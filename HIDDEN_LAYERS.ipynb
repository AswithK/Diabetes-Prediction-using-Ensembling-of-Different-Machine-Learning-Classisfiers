{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7MsAIp0Y3Nc",
        "outputId": "305a93fb-dde9-4c86-d277-f7026084b27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (16.0.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (2.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (0.3.25)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (67.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (2.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (23.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (0.32.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (1.53.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Collecting numpy<1.24,>=1.22\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.10.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.17.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (6.3.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Installing collected packages: numpy, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.2\n",
            "    Uninstalling numpy-1.24.2:\n",
            "      Successfully uninstalled numpy-1.24.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\n",
            "flax 0.6.8 requires jax>=0.4.2, but you have jax 0.3.25 which is incompatible.\n",
            "chex 0.1.7 requires jax>=0.4.6, but you have jax 0.3.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5 tensorflow-2.12.0\n"
          ]
        }
      ],
      "source": [
        " !pip uninstall tensorflow\n",
        " !pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw22MW22BX1h",
        "outputId": "6f48a404-8d4f-4640-ddf5-7f69c5db5b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xz80SgiJtd5o"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "np.random.seed(6)\n",
        "import random\n",
        "random.seed(6)\n",
        "# from keras import backend as K\n",
        "from tensorflow.random import set_seed\n",
        "set_seed(6)\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# sns.set(style=\"whitegrid\")\n",
        "import warnings \n",
        "from sklearn.svm import SVC, NuSVC\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors  import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "from scipy import stats\n",
        "from scipy.stats import uniform, randint\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
        "# from tflearn.data_utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy import interp\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import FastICA\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Activation, Dense, Dropout, BatchNormalization, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d272noMSoezZ"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B5ce3ePzKLDP"
      },
      "outputs": [],
      "source": [
        "Renamed_feature= []               \n",
        "all_clf_res=[]                    \n",
        "random_initializer=20            \n",
        "n_dots=50\n",
        "\n",
        "\n",
        "for i in range(8):\n",
        "  Renamed_feature.append('F'+str(i+1)) \n",
        "\n",
        "def Manual (data):\n",
        "\n",
        "    max_Pregnancies = data.F1.max()                         \n",
        "    data = data[data.F1!=max_Pregnancies]                   \n",
        "    max_Glucose = data.F2.max()                              \n",
        "    data = data[data.F2!=max_Glucose]                       \n",
        "    for i in range(4):                                      \n",
        "      min_Glucose = data.F2.min()                          \n",
        "      data = data[data.F2!=min_Glucose]                    \n",
        "    max_BloodPressure = data.F3.max()                      \n",
        "    data = data[data.F3!=max_BloodPressure]                \n",
        "    for i in range(2):                                        \n",
        "      max_skinthickness = data.F4.max() \n",
        "      data = data[data.F4!=max_skinthickness]\n",
        "    for i in range(25):                                     \n",
        "      max_Insulin = data.F5.max() \n",
        "      data = data[data.F5!=max_Insulin]\n",
        "    max_bmi = data.F6.max()\n",
        "    data = data[data.F6!=max_bmi]\n",
        "    for i in range(4):                                       \n",
        "      min_bmi = data.F6.min() \n",
        "      data = data[data.F6!=min_bmi]\n",
        "    for i in range(20):                                      \n",
        "      max_DiabetesPedigreeF = data.F7.max()\n",
        "      data = data[data.F7!=max_DiabetesPedigreeF]\n",
        "    for i in range(20):                                      \n",
        "      max_age = data.F8.max() \n",
        "      data = data[data.F8!=max_age]\n",
        "      df =data\n",
        "    return data\n",
        "\n",
        "def IQR_Mean (data):\n",
        "\n",
        "  for i in range(8): \n",
        "    x = data[Renamed_feature[i]]\n",
        "    Q1 = x.quantile(0.25)                                  \n",
        "    Q3 = x.quantile(0.75)                                  \n",
        "    IQR = Q3-Q1                                             \n",
        "    mean = x.mean()                                        \n",
        "    for j in range(569):                                  \n",
        "      temp = x[j]                                          \n",
        "      LW = (Q1 - 1.5 * IQR)                               \n",
        "      UW = (Q3 + 1.5 * IQR)                                 \n",
        "      if temp < LW:                                         \n",
        "        x[j] = mean\n",
        "      if temp > UW:                                        \n",
        "        x[j] = mean\n",
        "    data[Renamed_feature[i]] = x\n",
        "  return data\n",
        "\n",
        "def IQR_Median (data): \n",
        "  for i in range(8):\n",
        "    x = data[Renamed_feature[i]]\n",
        "    Q1 = x.quantile(0.25)\n",
        "    Q3 = x.quantile(0.75)\n",
        "    IQR = Q3-Q1\n",
        "    median = x.quantile(0.5)                               \n",
        "    for j in range(569):                                   \n",
        "      temp = x[j]\n",
        "      LW = (Q1 - 1.5 * IQR)\n",
        "      UW = (Q3 + 1.5 * IQR)\n",
        "      if temp < LW:                                        \n",
        "        x[j] = median\n",
        "      if temp > UW:\n",
        "        x[j] = median                                      \n",
        "    data[Renamed_feature[i]] = x\n",
        "  return data\n",
        "\n",
        "def IQR (data):\n",
        "  for i in range(8):                                        \n",
        "    Q1 = data[Renamed_feature[i]].quantile(0.25)\n",
        "    Q3 = data[Renamed_feature[i]].quantile(0.75)\n",
        "    IQR = Q3-Q1                                             \n",
        "    LW = (Q1 - 1.5 * IQR)                                   \n",
        "    UW = (Q3 + 1.5 * IQR)                                  \n",
        "    data = data[data[Renamed_feature[i]]<UW]               \n",
        "    data = data[data[Renamed_feature[i]]>LW]               \n",
        "\n",
        "  return data\n",
        "\n",
        "def outlier_Rejection (data, iqr_Mean, iqr_Medain, iqr, manual):\n",
        "  if iqr_Mean == True:                                   \n",
        "    data = IQR_Mean (data)\n",
        "  if iqr_Medain == True:                                  \n",
        "    data = IQR_Medain (data)\n",
        "  if iqr == True:                                        \n",
        "    data = IQR (data)\n",
        "  if manual == True:                                      \n",
        "    data = Manual (data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def replace_zero(data, field, target):\n",
        "    \n",
        "    mean_by_target = data.loc[data[field] != 0, [field, target]].groupby(target).mean()\n",
        "    data.loc[(data[field] == 0)&(data[target] == 0), field] = mean_by_target.iloc[0][0]\n",
        "    data.loc[(data[field] == 0)&(data[target] == 1), field] = mean_by_target.iloc[1][0]\n",
        "\n",
        "def metrics (y_true, y_pred, probas_):\n",
        "\n",
        "  points=n_dots*'-'\n",
        "  print(points)\n",
        "  fpr, tpr, thresholds = roc_curve(y_true, probas_[:, 1])\n",
        "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "  tprs[-1][0] = 0.0\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "  print(\"Detailed classification report for current fold:\")\n",
        "  print()\n",
        "  print(classification_report(y_true, y_pred))\n",
        "  print()\n",
        "  print(\"Area Under ROC (AUC): {}\".format(roc_auc))\n",
        "  print()\n",
        "  print('Confusion Matrix for current fold: ')\n",
        "  print(confusion_matrix(y_true, y_pred))\n",
        "  print()\n",
        "  print(\"Accuracy for Current Fold: {}\".format(accuracy_score(y_true, y_pred)))\n",
        "  print()\n",
        "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "  return  tn, fp, fn, tp, roc_auc, fpr, tpr\n",
        "\n",
        "\n",
        "def average_ROC(mean_fpr,tprs,aucs,TP,TN,FP,FN):\n",
        "\n",
        "  sen = (np.sum(TP))/(np.sum(TP)+np.sum(FN))\n",
        "  spe = (np.sum(TN))/(np.sum(TN)+np.sum(FP))\n",
        "\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_tpr[-1] = 1.0\n",
        "  mean_auc = np.mean(aucs)\n",
        "  std_auc = np.std(aucs)\n",
        "  ax = plt.axes()\n",
        "  ax.grid(color='lightgray', linestyle='-', linewidth=.5)\n",
        "  ax.set_facecolor(\"white\")\n",
        "  \n",
        "  ax.spines['bottom'].set_color('#000000')\n",
        "  ax.spines['top'].set_color('#000000') \n",
        "  ax.spines['right'].set_color('#000000')\n",
        "  ax.spines['left'].set_color('#000000')\n",
        "\n",
        "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
        "          label=r'Avg. ROC (AUC (avg $\\pm$ std) = %0.3f $\\pm$ %0.3f)' % (mean_auc, std_auc),\n",
        "          lw=2, alpha=.8)\n",
        "  \n",
        "  plt.scatter((1-spe), sen, s=80, c='r', marker='x',)\n",
        "  plt.scatter(0, sen, s=80, c='r', marker='x',)\n",
        "  plt.scatter((1-spe),0, s=80, c='r', marker='x',)\n",
        "  plt.axhline(y=sen, color='r', linestyle='--')\n",
        "  plt.axvline(x=(1-spe), color='r', linestyle='--')\n",
        "  plt.text((1-spe), 0.02, \"FPR={:0.3f}\".format((1-spe)))\n",
        "  plt.text(0.009, sen+0.05, \"TPR={:0.3f}\".format(sen))\n",
        "\n",
        "  std_tpr = np.std(tprs, axis=0)\n",
        "  tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
        "  tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
        "  plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='darkgray', alpha=0.5,\n",
        "                  label=r'$\\pm$ 1 Standard deviation')\n",
        "\n",
        "  plt.xticks(np.arange(0.0, 1.01, step=0.1))\n",
        "  plt.yticks(np.arange(0.0, 1.01, step=0.1))\n",
        "  left=0.0\n",
        "  right=1.0\n",
        "  plt.xlim(left, right)\n",
        "  plt.ylim(left, right)\n",
        "  plt.xlabel('False Positive Rate (FPR)')\n",
        "  plt.ylabel('True Positive Rate (TPR)')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.show()\n",
        "def plot_Current_ROC(fpr,tpr,iterator,roc_auc):\n",
        "\n",
        "  plt.plot(fpr,\n",
        "          \n",
        "          tpr,\n",
        "          alpha=0.35,\n",
        "          linewidth=1)\n",
        "def creat_Model (classifier, X_Train, Y_Train, tuned_parameters, verbose):\n",
        "\n",
        "  clf = GridSearchCV(classifier,\n",
        "                    tuned_parameters,\n",
        "                    verbose=verbose,\n",
        "                    cv=5,\n",
        "                    scoring='roc_auc',\n",
        "                    n_jobs=-1)\n",
        "  clf.fit(X_Train, Y_Train)\n",
        "  return clf\n",
        "\n",
        "def average_performance(aucs,Accuracy,TP,TN,FP,FN): \n",
        "\n",
        "  print()\n",
        "  n_dotsav=(n_dots-len('Average'))//2\n",
        "    \n",
        "  print('-'*n_dotsav+'Average'+'-'*n_dotsav)\n",
        "  print(\"AUC (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(aucs),np.std(aucs)))\n",
        "  print(\"Accuracy (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(Accuracy),np.std(Accuracy)))\n",
        "  cm = [[int(np.mean(TP)), int(np.mean(FP))],[int(np.mean(FN)), int(np.mean(TN))]]\n",
        "  print ('Avg. CM is '+str(cm))\n",
        "  cm = [[int(np.sum(TP)), int(np.sum(FP))],[int(np.sum(FN)), int(np.sum(TN))]]\n",
        "  print ('Total for all folds CM is '+str(cm))\n",
        "  re_auc=str(round(np.mean(aucs), 3))+'+/-'+str(round(np.std(aucs),3))\n",
        "  all_clf_res.append(re_auc)\n",
        "    \n",
        "def feature_Selector(data, algo, n_feature):\n",
        "\n",
        "    if algo=='PCA':                                                   \n",
        "        X_Data= data.iloc[:,:8].values\n",
        "        pca = PCA(n_components=n_feature)                            \n",
        "        X_Data = pca.fit_transform(X_Data)\n",
        "        return X_Data , data.iloc[:,8:].values\n",
        "\n",
        "    if algo == 'ICA':\n",
        "        X_Data= data.iloc[:,:8].values\n",
        "        ICA = FastICA(n_components=n_feature, random_state=12) \n",
        "        X_Data = ICA.fit_transform(X_Data)\n",
        "        return X_Data , data.iloc[:,8:].values\n",
        "    \n",
        "    if algo =='corr':                                                  \n",
        "        if n_feature ==4:\n",
        "            data = data[['F2','F5','F4','F6','Outcome']]               \n",
        "            return data.iloc[:,:4].values, data.iloc[:,4:].values\n",
        "        if n_feature ==6:\n",
        "            data = data[['F1','F2','F4','F5','F6','F8','Outcome']]      \n",
        "            return data.iloc[:,:6].values, data.iloc[:,6:].values\n",
        "        \n",
        "    if algo == 'None':\n",
        "        return data.iloc[:,:8].values, data.iloc[:,8:].values            \n",
        "    \n",
        "    \n",
        "\n",
        "def nn_opt(activation,dropout_rate,init,learn_rate):\n",
        "\n",
        "    neuron1,neuron2,neuron3,neuron4,neuron5=64,16,64,64,64\n",
        "    model = Sequential()\n",
        "    np.random.seed(6)\n",
        "    model.add(Dense(neuron1, input_dim =4 , kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron4, input_dim = neuron3, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dense(neuron5, input_dim = neuron4, kernel_initializer= init, activation= activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    \n",
        "    optimizer = Adam(lr = learn_rate)              \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
        "    return model     \n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Xr_D57O2I3_a",
        "outputId": "d8d273d8-7671-46b2-cd3e-672f12e7fb00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-23bc3122-7723-455f-8538-90fd1c020f24\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-23bc3122-7723-455f-8538-90fd1c020f24\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Dia.csv to Dia.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv(\"Dia.csv\")"
      ],
      "metadata": {
        "id": "PjP5MlUKJMJR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "KluoAwn49hGG",
        "outputId": "fc6eb81c-a1b1-44b1-b51e-2806f91fb290"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PatientID  Pregnancies  PlasmaGlucose  DiastolicBloodPressure  Unnamed: 4  \\\n",
              "0    1354778            0            171                      80         NaN   \n",
              "1    1147438            8             92                      93         NaN   \n",
              "2    1640031            7            115                      47         NaN   \n",
              "3    1883350            9            103                      78         NaN   \n",
              "4    1424119            1             85                      59         NaN   \n",
              "5    1619297            0             82                      92         NaN   \n",
              "\n",
              "   TricepsThickness  SerumInsulin        BMI  DiabetesPedigree  Age  Diabetic  \n",
              "0                34            23  43.509726          1.213191   21         0  \n",
              "1                47            36  21.240576          0.158365   23         0  \n",
              "2                52            35  41.511523          0.079019   23         0  \n",
              "3                25           304  29.582192          1.282870   43         1  \n",
              "4                27            35  42.604536          0.549542   22         0  \n",
              "5                 9           253  19.724160          0.103424   26         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09891755-61dd-43e6-996e-590a13118673\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PatientID</th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>PlasmaGlucose</th>\n",
              "      <th>DiastolicBloodPressure</th>\n",
              "      <th>Unnamed: 4</th>\n",
              "      <th>TricepsThickness</th>\n",
              "      <th>SerumInsulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigree</th>\n",
              "      <th>Age</th>\n",
              "      <th>Diabetic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1354778</td>\n",
              "      <td>0</td>\n",
              "      <td>171</td>\n",
              "      <td>80</td>\n",
              "      <td>NaN</td>\n",
              "      <td>34</td>\n",
              "      <td>23</td>\n",
              "      <td>43.509726</td>\n",
              "      <td>1.213191</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1147438</td>\n",
              "      <td>8</td>\n",
              "      <td>92</td>\n",
              "      <td>93</td>\n",
              "      <td>NaN</td>\n",
              "      <td>47</td>\n",
              "      <td>36</td>\n",
              "      <td>21.240576</td>\n",
              "      <td>0.158365</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1640031</td>\n",
              "      <td>7</td>\n",
              "      <td>115</td>\n",
              "      <td>47</td>\n",
              "      <td>NaN</td>\n",
              "      <td>52</td>\n",
              "      <td>35</td>\n",
              "      <td>41.511523</td>\n",
              "      <td>0.079019</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1883350</td>\n",
              "      <td>9</td>\n",
              "      <td>103</td>\n",
              "      <td>78</td>\n",
              "      <td>NaN</td>\n",
              "      <td>25</td>\n",
              "      <td>304</td>\n",
              "      <td>29.582192</td>\n",
              "      <td>1.282870</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1424119</td>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>59</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27</td>\n",
              "      <td>35</td>\n",
              "      <td>42.604536</td>\n",
              "      <td>0.549542</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1619297</td>\n",
              "      <td>0</td>\n",
              "      <td>82</td>\n",
              "      <td>92</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>253</td>\n",
              "      <td>19.724160</td>\n",
              "      <td>0.103424</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09891755-61dd-43e6-996e-590a13118673')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09891755-61dd-43e6-996e-590a13118673 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09891755-61dd-43e6-996e-590a13118673');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data.head(n=6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop('Unnamed: 4', axis=1)\n",
        "data = data.drop('PatientID', axis=1)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "pyF4EGBvJWxv",
        "outputId": "e8e26d31-2238-4a73-8638-263caa17808b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Pregnancies  PlasmaGlucose  DiastolicBloodPressure  TricepsThickness  \\\n",
              "0                0            171                      80                34   \n",
              "1                8             92                      93                47   \n",
              "2                7            115                      47                52   \n",
              "3                9            103                      78                25   \n",
              "4                1             85                      59                27   \n",
              "...            ...            ...                     ...               ...   \n",
              "14890           10             65                      60                46   \n",
              "14891            2             73                      66                27   \n",
              "14892            0             93                      89                43   \n",
              "14893            0            132                      98                18   \n",
              "14894            3            114                      65                47   \n",
              "\n",
              "       SerumInsulin        BMI  DiabetesPedigree  Age  Diabetic  \n",
              "0                23  43.509726          1.213191   21         0  \n",
              "1                36  21.240576          0.158365   23         0  \n",
              "2                35  41.511523          0.079019   23         0  \n",
              "3               304  29.582192          1.282870   43         1  \n",
              "4                35  42.604536          0.549542   22         0  \n",
              "...             ...        ...               ...  ...       ...  \n",
              "14890           177  33.512468          0.148327   41         1  \n",
              "14891           168  30.132636          0.862252   38         1  \n",
              "14892            57  18.690683          0.427049   24         0  \n",
              "14893           161  19.791645          0.302257   23         0  \n",
              "14894           512  36.215437          0.147363   34         1  \n",
              "\n",
              "[14895 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5bdf83d-91a9-49bd-9e69-d06e0b567b61\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>PlasmaGlucose</th>\n",
              "      <th>DiastolicBloodPressure</th>\n",
              "      <th>TricepsThickness</th>\n",
              "      <th>SerumInsulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigree</th>\n",
              "      <th>Age</th>\n",
              "      <th>Diabetic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>171</td>\n",
              "      <td>80</td>\n",
              "      <td>34</td>\n",
              "      <td>23</td>\n",
              "      <td>43.509726</td>\n",
              "      <td>1.213191</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>92</td>\n",
              "      <td>93</td>\n",
              "      <td>47</td>\n",
              "      <td>36</td>\n",
              "      <td>21.240576</td>\n",
              "      <td>0.158365</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>115</td>\n",
              "      <td>47</td>\n",
              "      <td>52</td>\n",
              "      <td>35</td>\n",
              "      <td>41.511523</td>\n",
              "      <td>0.079019</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>103</td>\n",
              "      <td>78</td>\n",
              "      <td>25</td>\n",
              "      <td>304</td>\n",
              "      <td>29.582192</td>\n",
              "      <td>1.282870</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>59</td>\n",
              "      <td>27</td>\n",
              "      <td>35</td>\n",
              "      <td>42.604536</td>\n",
              "      <td>0.549542</td>\n",
              "      <td>22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14890</th>\n",
              "      <td>10</td>\n",
              "      <td>65</td>\n",
              "      <td>60</td>\n",
              "      <td>46</td>\n",
              "      <td>177</td>\n",
              "      <td>33.512468</td>\n",
              "      <td>0.148327</td>\n",
              "      <td>41</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14891</th>\n",
              "      <td>2</td>\n",
              "      <td>73</td>\n",
              "      <td>66</td>\n",
              "      <td>27</td>\n",
              "      <td>168</td>\n",
              "      <td>30.132636</td>\n",
              "      <td>0.862252</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14892</th>\n",
              "      <td>0</td>\n",
              "      <td>93</td>\n",
              "      <td>89</td>\n",
              "      <td>43</td>\n",
              "      <td>57</td>\n",
              "      <td>18.690683</td>\n",
              "      <td>0.427049</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14893</th>\n",
              "      <td>0</td>\n",
              "      <td>132</td>\n",
              "      <td>98</td>\n",
              "      <td>18</td>\n",
              "      <td>161</td>\n",
              "      <td>19.791645</td>\n",
              "      <td>0.302257</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14894</th>\n",
              "      <td>3</td>\n",
              "      <td>114</td>\n",
              "      <td>65</td>\n",
              "      <td>47</td>\n",
              "      <td>512</td>\n",
              "      <td>36.215437</td>\n",
              "      <td>0.147363</td>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14895 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5bdf83d-91a9-49bd-9e69-d06e0b567b61')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a5bdf83d-91a9-49bd-9e69-d06e0b567b61 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a5bdf83d-91a9-49bd-9e69-d06e0b567b61');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VvHHG7dC_jzd"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame({'F1':data.iloc[:,:8].values[:,0],\n",
        "                     'F2':data.iloc[:,:8].values[:,1],\n",
        "                     'F3':data.iloc[:,:8].values[:,2],\n",
        "                     'F4':data.iloc[:,:8].values[:,3],\n",
        "                     'F5':data.iloc[:,:8].values[:,4],\n",
        "                     'F6':data.iloc[:,:8].values[:,5],\n",
        "                     'F7':data.iloc[:,:8].values[:,6],\n",
        "                     'F8':data.iloc[:,:8].values[:,7],\n",
        "                     'Outcome':data.iloc[:,8:].values[:,0]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRoQ8jmU-f2A"
      },
      "source": [
        "### Show the statistical description of the data which sumarize the central tendency, dispersion, and shape of a data distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PdYuOfaHtpyn",
        "outputId": "b2fbaddc-d85d-4122-bbd0-1aa1b6f64f9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 F1            F2            F3            F4            F5  \\\n",
              "count  14895.000000  14895.000000  14895.000000  14895.000000  14895.000000   \n",
              "mean       3.224841    107.868278     71.238469     28.804162    137.957637   \n",
              "std        3.391987     32.002972     16.754499     14.555187    133.120086   \n",
              "min        0.000000     44.000000     24.000000      7.000000     14.000000   \n",
              "25%        0.000000     84.000000     58.000000     15.000000     39.000000   \n",
              "50%        2.000000    104.000000     72.000000     30.000000     83.000000   \n",
              "75%        6.000000    129.000000     85.000000     41.000000    195.000000   \n",
              "max       14.000000    192.000000    117.000000     93.000000    799.000000   \n",
              "\n",
              "                 F6            F7            F8       Outcome  \n",
              "count  14895.000000  14895.000000  14895.000000  14895.000000  \n",
              "mean      31.514957      0.398842     30.138234      0.333199  \n",
              "std        9.759037      0.378169     12.089241      0.471373  \n",
              "min       18.200512      0.078044     21.000000      0.000000  \n",
              "25%       21.259603      0.137695     22.000000      0.000000  \n",
              "50%       31.772471      0.200038     24.000000      0.000000  \n",
              "75%       39.254649      0.615603     35.000000      1.000000  \n",
              "max       56.034628      2.301594     77.000000      1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9dc7a217-34ff-4e18-ad11-a4e8b473e3ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>F1</th>\n",
              "      <th>F2</th>\n",
              "      <th>F3</th>\n",
              "      <th>F4</th>\n",
              "      <th>F5</th>\n",
              "      <th>F6</th>\n",
              "      <th>F7</th>\n",
              "      <th>F8</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "      <td>14895.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.224841</td>\n",
              "      <td>107.868278</td>\n",
              "      <td>71.238469</td>\n",
              "      <td>28.804162</td>\n",
              "      <td>137.957637</td>\n",
              "      <td>31.514957</td>\n",
              "      <td>0.398842</td>\n",
              "      <td>30.138234</td>\n",
              "      <td>0.333199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.391987</td>\n",
              "      <td>32.002972</td>\n",
              "      <td>16.754499</td>\n",
              "      <td>14.555187</td>\n",
              "      <td>133.120086</td>\n",
              "      <td>9.759037</td>\n",
              "      <td>0.378169</td>\n",
              "      <td>12.089241</td>\n",
              "      <td>0.471373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>18.200512</td>\n",
              "      <td>0.078044</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>21.259603</td>\n",
              "      <td>0.137695</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>31.772471</td>\n",
              "      <td>0.200038</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>129.000000</td>\n",
              "      <td>85.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>39.254649</td>\n",
              "      <td>0.615603</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.000000</td>\n",
              "      <td>192.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>799.000000</td>\n",
              "      <td>56.034628</td>\n",
              "      <td>2.301594</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9dc7a217-34ff-4e18-ad11-a4e8b473e3ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9dc7a217-34ff-4e18-ad11-a4e8b473e3ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9dc7a217-34ff-4e18-ad11-a4e8b473e3ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7790af18-672d-658b-9d0a-58648dd63b13",
        "id": "nluf1OUetXjy"
      },
      "source": [
        "# Data Preprocessing  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_cell_guid": "048157d7-1cfc-13b7-feca-5c73b00a03ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbTObgkctXj1",
        "outputId": "381e7918-8fcb-46d2-ab03-2a5290d6a1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape Before Process: (14895, 9)\n",
            "Shape After outlier Removed: (12562, 9)\n",
            "Shape After Filling Missing Value: (12562, 9)\n",
            "Shape After Feature Selection: (12562, 4)\n"
          ]
        }
      ],
      "source": [
        "print('Shape Before Process: ' + str(data.shape))\n",
        "\n",
        "data = outlier_Rejection (data,\n",
        "                  iqr_Mean=False,\n",
        "                  iqr_Medain=False,\n",
        "                  iqr=True,\n",
        "                  manual=False)\n",
        "print('Shape After outlier Removed: ' + str(data.shape))\n",
        "\n",
        "for col in ['F2', 'F3', 'F4', 'F5', 'F6']:   \n",
        "    replace_zero(data, col, 'Outcome')              \n",
        "print('Shape After Filling Missing Value: ' + str(data.shape))\n",
        "     \n",
        "                        \n",
        "X_Data,Y_Lavel = feature_Selector(data, algo='corr', n_feature=4)    \n",
        "print('Shape After Feature Selection: ' + str(X_Data.shape))    \n",
        "\n",
        "kf = StratifiedKFold(n_splits=5,\n",
        "                     shuffle=True,\n",
        "                     random_state=random_initializer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9kt1_ZOnSWm"
      },
      "source": [
        "### MLP Experiment Block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LyVnsEj3RE_M"
      },
      "outputs": [],
      "source": [
        "\n",
        "seed = 6\n",
        "np.random.seed(seed)\n",
        "model = KerasClassifier(build_fn = nn_opt, verbose = 0)\n",
        "\n",
        "batch_size = [8, 16, 32]            \n",
        "epochs = [100, 150, 200]              \n",
        "learn_rate =[0.001,.05, 0.1]         \n",
        "dropout_rate = [0.0, 0.3, 0.6]       \n",
        "activation = ['relu', 'tanh']   \n",
        "init =['uniform', 'normal']         \n",
        "param_grid = dict(batch_size=batch_size,\n",
        "                  epochs=epochs,\n",
        "                  learn_rate=learn_rate,\n",
        "                  dropout_rate=dropout_rate,\n",
        "                  activation=activation,\n",
        "                  init=init)\n",
        "\n",
        "grid = GridSearchCV(estimator = model,\n",
        "                    param_grid = param_grid,\n",
        "                    cv = 5,\n",
        "                    n_jobs = -1,\n",
        "                    verbose = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UPZJ_TGUh9Q_",
        "outputId": "df6ae662-1223-4d3a-ad25-f53253b177f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "relu 8 200 0.001 0.6 normal 64 16 64 64\n",
            "------------------->>>>>>>>>>Fold no =  1\n",
            "Epoch 1/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.5604 - accuracy: 0.7257\n",
            "Epoch 2/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.5269 - accuracy: 0.7308\n",
            "Epoch 3/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.5034 - accuracy: 0.7503\n",
            "Epoch 4/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4875 - accuracy: 0.7647\n",
            "Epoch 5/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4891 - accuracy: 0.7614\n",
            "Epoch 6/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4826 - accuracy: 0.7681\n",
            "Epoch 7/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4785 - accuracy: 0.7671\n",
            "Epoch 8/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4776 - accuracy: 0.7719\n",
            "Epoch 9/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4707 - accuracy: 0.7736\n",
            "Epoch 10/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4668 - accuracy: 0.7744\n",
            "Epoch 11/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4549 - accuracy: 0.7812\n",
            "Epoch 12/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4571 - accuracy: 0.7824\n",
            "Epoch 13/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4484 - accuracy: 0.7901\n",
            "Epoch 14/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4462 - accuracy: 0.7903\n",
            "Epoch 15/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4446 - accuracy: 0.7924\n",
            "Epoch 16/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4419 - accuracy: 0.8024\n",
            "Epoch 17/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4373 - accuracy: 0.8019\n",
            "Epoch 18/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4345 - accuracy: 0.8086\n",
            "Epoch 19/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4299 - accuracy: 0.8118\n",
            "Epoch 20/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4288 - accuracy: 0.8138\n",
            "Epoch 21/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4306 - accuracy: 0.8113\n",
            "Epoch 22/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4263 - accuracy: 0.8101\n",
            "Epoch 23/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4267 - accuracy: 0.8153\n",
            "Epoch 24/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4259 - accuracy: 0.8136\n",
            "Epoch 25/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4240 - accuracy: 0.8171\n",
            "Epoch 26/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4263 - accuracy: 0.8137\n",
            "Epoch 27/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4242 - accuracy: 0.8147\n",
            "Epoch 28/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4184 - accuracy: 0.8185\n",
            "Epoch 29/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4239 - accuracy: 0.8129\n",
            "Epoch 30/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4220 - accuracy: 0.8147\n",
            "Epoch 31/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4225 - accuracy: 0.8138\n",
            "Epoch 32/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4213 - accuracy: 0.8157\n",
            "Epoch 33/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4200 - accuracy: 0.8141\n",
            "Epoch 34/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4180 - accuracy: 0.8155\n",
            "Epoch 35/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4182 - accuracy: 0.8131\n",
            "Epoch 36/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4161 - accuracy: 0.8193\n",
            "Epoch 37/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4174 - accuracy: 0.8149\n",
            "Epoch 38/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4148 - accuracy: 0.8178\n",
            "Epoch 39/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4149 - accuracy: 0.8193\n",
            "Epoch 40/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4160 - accuracy: 0.8191\n",
            "Epoch 41/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4165 - accuracy: 0.8165\n",
            "Epoch 42/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4172 - accuracy: 0.8205\n",
            "Epoch 43/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4159 - accuracy: 0.8156\n",
            "Epoch 44/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4139 - accuracy: 0.8191\n",
            "Epoch 45/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4146 - accuracy: 0.8186\n",
            "Epoch 46/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4123 - accuracy: 0.8180\n",
            "Epoch 47/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4243 - accuracy: 0.8147\n",
            "Epoch 48/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4132 - accuracy: 0.8220\n",
            "Epoch 49/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4112 - accuracy: 0.8191\n",
            "Epoch 50/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4184 - accuracy: 0.8197\n",
            "Epoch 51/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4289 - accuracy: 0.8120\n",
            "Epoch 52/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4233 - accuracy: 0.8137\n",
            "Epoch 53/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4193 - accuracy: 0.8166\n",
            "Epoch 54/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4170 - accuracy: 0.8189\n",
            "Epoch 55/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4176 - accuracy: 0.8158\n",
            "Epoch 56/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4154 - accuracy: 0.8143\n",
            "Epoch 57/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4219 - accuracy: 0.8155\n",
            "Epoch 58/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4182 - accuracy: 0.8147\n",
            "Epoch 59/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4254 - accuracy: 0.8111\n",
            "Epoch 60/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4154 - accuracy: 0.8170\n",
            "Epoch 61/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4106 - accuracy: 0.8214\n",
            "Epoch 62/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4130 - accuracy: 0.8214\n",
            "Epoch 63/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4101 - accuracy: 0.8262\n",
            "Epoch 64/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4116 - accuracy: 0.8156\n",
            "Epoch 65/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4064 - accuracy: 0.8225\n",
            "Epoch 66/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4066 - accuracy: 0.8234\n",
            "Epoch 67/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4112 - accuracy: 0.8192\n",
            "Epoch 68/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4048 - accuracy: 0.8223\n",
            "Epoch 69/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4123 - accuracy: 0.8226\n",
            "Epoch 70/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4109 - accuracy: 0.8194\n",
            "Epoch 71/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4084 - accuracy: 0.8255\n",
            "Epoch 72/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4074 - accuracy: 0.8219\n",
            "Epoch 73/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4066 - accuracy: 0.8243\n",
            "Epoch 74/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4051 - accuracy: 0.8242\n",
            "Epoch 75/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4072 - accuracy: 0.8238\n",
            "Epoch 76/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4122 - accuracy: 0.8184\n",
            "Epoch 77/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4133 - accuracy: 0.8206\n",
            "Epoch 78/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4073 - accuracy: 0.8228\n",
            "Epoch 79/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4095 - accuracy: 0.8211\n",
            "Epoch 80/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4068 - accuracy: 0.8246\n",
            "Epoch 81/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4057 - accuracy: 0.8255\n",
            "Epoch 82/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4040 - accuracy: 0.8234\n",
            "Epoch 83/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4041 - accuracy: 0.8225\n",
            "Epoch 84/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4043 - accuracy: 0.8251\n",
            "Epoch 85/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4002 - accuracy: 0.8267\n",
            "Epoch 86/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4032 - accuracy: 0.8217\n",
            "Epoch 87/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4028 - accuracy: 0.8241\n",
            "Epoch 88/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3998 - accuracy: 0.8247\n",
            "Epoch 89/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4035 - accuracy: 0.8228\n",
            "Epoch 90/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4042 - accuracy: 0.8242\n",
            "Epoch 91/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4006 - accuracy: 0.8238\n",
            "Epoch 92/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3971 - accuracy: 0.8261\n",
            "Epoch 93/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4039 - accuracy: 0.8261\n",
            "Epoch 94/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3971 - accuracy: 0.8259\n",
            "Epoch 95/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3991 - accuracy: 0.8283\n",
            "Epoch 96/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4029 - accuracy: 0.8246\n",
            "Epoch 97/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4019 - accuracy: 0.8238\n",
            "Epoch 98/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3993 - accuracy: 0.8261\n",
            "Epoch 99/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4035 - accuracy: 0.8228\n",
            "Epoch 100/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4021 - accuracy: 0.8270\n",
            "Epoch 101/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4005 - accuracy: 0.8278\n",
            "Epoch 102/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3961 - accuracy: 0.8246\n",
            "Epoch 103/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4010 - accuracy: 0.8297\n",
            "Epoch 104/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3967 - accuracy: 0.8299\n",
            "Epoch 105/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4007 - accuracy: 0.8211\n",
            "Epoch 106/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4042 - accuracy: 0.8230\n",
            "Epoch 107/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3981 - accuracy: 0.8262\n",
            "Epoch 108/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3995 - accuracy: 0.8238\n",
            "Epoch 109/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3972 - accuracy: 0.8242\n",
            "Epoch 110/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4008 - accuracy: 0.8232\n",
            "Epoch 111/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3975 - accuracy: 0.8277\n",
            "Epoch 112/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4030 - accuracy: 0.8236\n",
            "Epoch 113/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3977 - accuracy: 0.8277\n",
            "Epoch 114/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3970 - accuracy: 0.8293\n",
            "Epoch 115/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4043 - accuracy: 0.8218\n",
            "Epoch 116/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3950 - accuracy: 0.8257\n",
            "Epoch 117/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3984 - accuracy: 0.8269\n",
            "Epoch 118/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3982 - accuracy: 0.8240\n",
            "Epoch 119/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3970 - accuracy: 0.8231\n",
            "Epoch 120/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3977 - accuracy: 0.8257\n",
            "Epoch 121/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3987 - accuracy: 0.8275\n",
            "Epoch 122/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3957 - accuracy: 0.8263\n",
            "Epoch 123/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4260 - accuracy: 0.8140\n",
            "Epoch 124/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4092 - accuracy: 0.8228\n",
            "Epoch 125/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4060 - accuracy: 0.8234\n",
            "Epoch 126/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3959 - accuracy: 0.8250\n",
            "Epoch 127/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3956 - accuracy: 0.8277\n",
            "Epoch 128/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3999 - accuracy: 0.8240\n",
            "Epoch 129/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3945 - accuracy: 0.8301\n",
            "Epoch 130/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4004 - accuracy: 0.8241\n",
            "Epoch 131/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3932 - accuracy: 0.8275\n",
            "Epoch 132/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4003 - accuracy: 0.8249\n",
            "Epoch 133/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4043 - accuracy: 0.8218\n",
            "Epoch 134/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4053 - accuracy: 0.8214\n",
            "Epoch 135/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3984 - accuracy: 0.8289\n",
            "Epoch 136/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3987 - accuracy: 0.8282\n",
            "Epoch 137/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3957 - accuracy: 0.8305\n",
            "Epoch 138/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3924 - accuracy: 0.8265\n",
            "Epoch 139/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3957 - accuracy: 0.8292\n",
            "Epoch 140/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3977 - accuracy: 0.8268\n",
            "Epoch 141/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3994 - accuracy: 0.8296\n",
            "Epoch 142/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3970 - accuracy: 0.8282\n",
            "Epoch 143/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3939 - accuracy: 0.8303\n",
            "Epoch 144/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3964 - accuracy: 0.8279\n",
            "Epoch 145/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3952 - accuracy: 0.8275\n",
            "Epoch 146/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3946 - accuracy: 0.8265\n",
            "Epoch 147/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3973 - accuracy: 0.8274\n",
            "Epoch 148/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3969 - accuracy: 0.8291\n",
            "Epoch 149/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3934 - accuracy: 0.8287\n",
            "Epoch 150/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3925 - accuracy: 0.8306\n",
            "Epoch 151/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3949 - accuracy: 0.8283\n",
            "Epoch 152/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3947 - accuracy: 0.8272\n",
            "Epoch 153/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3926 - accuracy: 0.8298\n",
            "Epoch 154/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3948 - accuracy: 0.8301\n",
            "Epoch 155/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3929 - accuracy: 0.8282\n",
            "Epoch 156/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3949 - accuracy: 0.8302\n",
            "Epoch 157/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3938 - accuracy: 0.8272\n",
            "Epoch 158/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3910 - accuracy: 0.8293\n",
            "Epoch 159/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3939 - accuracy: 0.8264\n",
            "Epoch 160/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3957 - accuracy: 0.8288\n",
            "Epoch 161/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3882 - accuracy: 0.8297\n",
            "Epoch 162/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3977 - accuracy: 0.8289\n",
            "Epoch 163/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3950 - accuracy: 0.8279\n",
            "Epoch 164/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3967 - accuracy: 0.8245\n",
            "Epoch 165/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3924 - accuracy: 0.8316\n",
            "Epoch 166/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3912 - accuracy: 0.8327\n",
            "Epoch 167/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3931 - accuracy: 0.8263\n",
            "Epoch 168/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3898 - accuracy: 0.8322\n",
            "Epoch 169/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3932 - accuracy: 0.8287\n",
            "Epoch 170/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3890 - accuracy: 0.8323\n",
            "Epoch 171/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3888 - accuracy: 0.8331\n",
            "Epoch 172/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3906 - accuracy: 0.8323\n",
            "Epoch 173/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3910 - accuracy: 0.8282\n",
            "Epoch 174/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3915 - accuracy: 0.8311\n",
            "Epoch 175/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3915 - accuracy: 0.8282\n",
            "Epoch 176/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3879 - accuracy: 0.8322\n",
            "Epoch 177/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3916 - accuracy: 0.8296\n",
            "Epoch 178/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3919 - accuracy: 0.8298\n",
            "Epoch 179/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3924 - accuracy: 0.8300\n",
            "Epoch 180/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3885 - accuracy: 0.8314\n",
            "Epoch 181/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3942 - accuracy: 0.8301\n",
            "Epoch 182/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3912 - accuracy: 0.8299\n",
            "Epoch 183/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3933 - accuracy: 0.8292\n",
            "Epoch 184/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3937 - accuracy: 0.8308\n",
            "Epoch 185/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3872 - accuracy: 0.8319\n",
            "Epoch 186/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3907 - accuracy: 0.8296\n",
            "Epoch 187/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3903 - accuracy: 0.8300\n",
            "Epoch 188/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4027 - accuracy: 0.8267\n",
            "Epoch 189/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3927 - accuracy: 0.8304\n",
            "Epoch 190/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3879 - accuracy: 0.8361\n",
            "Epoch 191/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3881 - accuracy: 0.8333\n",
            "Epoch 192/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3901 - accuracy: 0.8308\n",
            "Epoch 193/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3863 - accuracy: 0.8318\n",
            "Epoch 194/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3938 - accuracy: 0.8279\n",
            "Epoch 195/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4332 - accuracy: 0.7966\n",
            "Epoch 196/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4240 - accuracy: 0.8011\n",
            "Epoch 197/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4175 - accuracy: 0.8046\n",
            "Epoch 198/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4147 - accuracy: 0.8129\n",
            "Epoch 199/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4146 - accuracy: 0.8208\n",
            "Epoch 200/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4070 - accuracy: 0.8225\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86      1824\n",
            "           1       0.62      0.60      0.61       689\n",
            "\n",
            "    accuracy                           0.79      2513\n",
            "   macro avg       0.73      0.73      0.73      2513\n",
            "weighted avg       0.79      0.79      0.79      2513\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8286589227968324\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[1571  253]\n",
            " [ 278  411]]\n",
            "\n",
            "Accuracy for Current Fold: 0.7886987664146439\n",
            "\n",
            "9.180211561975716\n",
            "------------------->>>>>>>>>>Fold no =  2\n",
            "Epoch 1/200\n",
            "1257/1257 [==============================] - 5s 3ms/step - loss: 0.5642 - accuracy: 0.7259\n",
            "Epoch 2/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.5386 - accuracy: 0.7264\n",
            "Epoch 3/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5120 - accuracy: 0.7427\n",
            "Epoch 4/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4999 - accuracy: 0.7560\n",
            "Epoch 5/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4921 - accuracy: 0.7578\n",
            "Epoch 6/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4892 - accuracy: 0.7613\n",
            "Epoch 7/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4858 - accuracy: 0.7577\n",
            "Epoch 8/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4814 - accuracy: 0.7634\n",
            "Epoch 9/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4786 - accuracy: 0.7666\n",
            "Epoch 10/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4766 - accuracy: 0.7658\n",
            "Epoch 11/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4736 - accuracy: 0.7699\n",
            "Epoch 12/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4660 - accuracy: 0.7736\n",
            "Epoch 13/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4665 - accuracy: 0.7738\n",
            "Epoch 14/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4670 - accuracy: 0.7718\n",
            "Epoch 15/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4617 - accuracy: 0.7748\n",
            "Epoch 16/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4618 - accuracy: 0.7761\n",
            "Epoch 17/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4587 - accuracy: 0.7769\n",
            "Epoch 18/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4619 - accuracy: 0.7772\n",
            "Epoch 19/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4539 - accuracy: 0.7827\n",
            "Epoch 20/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4516 - accuracy: 0.7820\n",
            "Epoch 21/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4493 - accuracy: 0.7852\n",
            "Epoch 22/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4556 - accuracy: 0.7848\n",
            "Epoch 23/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4492 - accuracy: 0.7894\n",
            "Epoch 24/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4519 - accuracy: 0.7849\n",
            "Epoch 25/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4475 - accuracy: 0.7935\n",
            "Epoch 26/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4398 - accuracy: 0.8002\n",
            "Epoch 27/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4361 - accuracy: 0.8000\n",
            "Epoch 28/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4316 - accuracy: 0.8061\n",
            "Epoch 29/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4333 - accuracy: 0.8068\n",
            "Epoch 30/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4305 - accuracy: 0.8072\n",
            "Epoch 31/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4263 - accuracy: 0.8097\n",
            "Epoch 32/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4321 - accuracy: 0.8052\n",
            "Epoch 33/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4339 - accuracy: 0.8059\n",
            "Epoch 34/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4264 - accuracy: 0.8120\n",
            "Epoch 35/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4329 - accuracy: 0.8099\n",
            "Epoch 36/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4251 - accuracy: 0.8114\n",
            "Epoch 37/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4271 - accuracy: 0.8112\n",
            "Epoch 38/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4254 - accuracy: 0.8112\n",
            "Epoch 39/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4246 - accuracy: 0.8106\n",
            "Epoch 40/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4281 - accuracy: 0.8090\n",
            "Epoch 41/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4258 - accuracy: 0.8108\n",
            "Epoch 42/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4236 - accuracy: 0.8138\n",
            "Epoch 43/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4259 - accuracy: 0.8097\n",
            "Epoch 44/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4238 - accuracy: 0.8093\n",
            "Epoch 45/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4218 - accuracy: 0.8111\n",
            "Epoch 46/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4209 - accuracy: 0.8171\n",
            "Epoch 47/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4233 - accuracy: 0.8113\n",
            "Epoch 48/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4189 - accuracy: 0.8148\n",
            "Epoch 49/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4176 - accuracy: 0.8134\n",
            "Epoch 50/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4192 - accuracy: 0.8146\n",
            "Epoch 51/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4163 - accuracy: 0.8140\n",
            "Epoch 52/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4196 - accuracy: 0.8132\n",
            "Epoch 53/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4172 - accuracy: 0.8176\n",
            "Epoch 54/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4188 - accuracy: 0.8152\n",
            "Epoch 55/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4160 - accuracy: 0.8134\n",
            "Epoch 56/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4183 - accuracy: 0.8118\n",
            "Epoch 57/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4169 - accuracy: 0.8149\n",
            "Epoch 58/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4152 - accuracy: 0.8149\n",
            "Epoch 59/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4110 - accuracy: 0.8168\n",
            "Epoch 60/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4113 - accuracy: 0.8167\n",
            "Epoch 61/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4155 - accuracy: 0.8164\n",
            "Epoch 62/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4141 - accuracy: 0.8174\n",
            "Epoch 63/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4137 - accuracy: 0.8174\n",
            "Epoch 64/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4142 - accuracy: 0.8182\n",
            "Epoch 65/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4129 - accuracy: 0.8174\n",
            "Epoch 66/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4107 - accuracy: 0.8170\n",
            "Epoch 67/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4138 - accuracy: 0.8157\n",
            "Epoch 68/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4146 - accuracy: 0.8156\n",
            "Epoch 69/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4133 - accuracy: 0.8153\n",
            "Epoch 70/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4168 - accuracy: 0.8150\n",
            "Epoch 71/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4092 - accuracy: 0.8192\n",
            "Epoch 72/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4111 - accuracy: 0.8173\n",
            "Epoch 73/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4073 - accuracy: 0.8163\n",
            "Epoch 74/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4120 - accuracy: 0.8151\n",
            "Epoch 75/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4111 - accuracy: 0.8141\n",
            "Epoch 76/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4111 - accuracy: 0.8183\n",
            "Epoch 77/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4147 - accuracy: 0.8168\n",
            "Epoch 78/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4079 - accuracy: 0.8202\n",
            "Epoch 79/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4104 - accuracy: 0.8178\n",
            "Epoch 80/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4070 - accuracy: 0.8196\n",
            "Epoch 81/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4083 - accuracy: 0.8176\n",
            "Epoch 82/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4123 - accuracy: 0.8159\n",
            "Epoch 83/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4128 - accuracy: 0.8135\n",
            "Epoch 84/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4128 - accuracy: 0.8184\n",
            "Epoch 85/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4132 - accuracy: 0.8179\n",
            "Epoch 86/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4055 - accuracy: 0.8205\n",
            "Epoch 87/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4065 - accuracy: 0.8191\n",
            "Epoch 88/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4066 - accuracy: 0.8204\n",
            "Epoch 89/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4070 - accuracy: 0.8226\n",
            "Epoch 90/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4050 - accuracy: 0.8212\n",
            "Epoch 91/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4076 - accuracy: 0.8214\n",
            "Epoch 92/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4064 - accuracy: 0.8212\n",
            "Epoch 93/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4068 - accuracy: 0.8201\n",
            "Epoch 94/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4052 - accuracy: 0.8224\n",
            "Epoch 95/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8198\n",
            "Epoch 96/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4007 - accuracy: 0.8222\n",
            "Epoch 97/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4064 - accuracy: 0.8233\n",
            "Epoch 98/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4000 - accuracy: 0.8230\n",
            "Epoch 99/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4069 - accuracy: 0.8203\n",
            "Epoch 100/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4028 - accuracy: 0.8241\n",
            "Epoch 101/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4048 - accuracy: 0.8223\n",
            "Epoch 102/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4053 - accuracy: 0.8207\n",
            "Epoch 103/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4035 - accuracy: 0.8222\n",
            "Epoch 104/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3996 - accuracy: 0.8248\n",
            "Epoch 105/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4075 - accuracy: 0.8178\n",
            "Epoch 106/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4087 - accuracy: 0.8199\n",
            "Epoch 107/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4026 - accuracy: 0.8209\n",
            "Epoch 108/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4046 - accuracy: 0.8198\n",
            "Epoch 109/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4029 - accuracy: 0.8227\n",
            "Epoch 110/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4039 - accuracy: 0.8202\n",
            "Epoch 111/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3995 - accuracy: 0.8217\n",
            "Epoch 112/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4000 - accuracy: 0.8231\n",
            "Epoch 113/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4039 - accuracy: 0.8240\n",
            "Epoch 114/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4023 - accuracy: 0.8204\n",
            "Epoch 115/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4008 - accuracy: 0.8219\n",
            "Epoch 116/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3954 - accuracy: 0.8250\n",
            "Epoch 117/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3993 - accuracy: 0.8219\n",
            "Epoch 118/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4000 - accuracy: 0.8229\n",
            "Epoch 119/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4002 - accuracy: 0.8234\n",
            "Epoch 120/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3991 - accuracy: 0.8234\n",
            "Epoch 121/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3975 - accuracy: 0.8237\n",
            "Epoch 122/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3987 - accuracy: 0.8211\n",
            "Epoch 123/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3962 - accuracy: 0.8233\n",
            "Epoch 124/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3978 - accuracy: 0.8228\n",
            "Epoch 125/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4025 - accuracy: 0.8224\n",
            "Epoch 126/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4003 - accuracy: 0.8214\n",
            "Epoch 127/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3995 - accuracy: 0.8263\n",
            "Epoch 128/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4041 - accuracy: 0.8224\n",
            "Epoch 129/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3997 - accuracy: 0.8252\n",
            "Epoch 130/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3985 - accuracy: 0.8217\n",
            "Epoch 131/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4006 - accuracy: 0.8236\n",
            "Epoch 132/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3960 - accuracy: 0.8258\n",
            "Epoch 133/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3949 - accuracy: 0.8235\n",
            "Epoch 134/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4002 - accuracy: 0.8240\n",
            "Epoch 135/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3987 - accuracy: 0.8248\n",
            "Epoch 136/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3938 - accuracy: 0.8245\n",
            "Epoch 137/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4012 - accuracy: 0.8225\n",
            "Epoch 138/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3968 - accuracy: 0.8229\n",
            "Epoch 139/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3998 - accuracy: 0.8216\n",
            "Epoch 140/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4004 - accuracy: 0.8224\n",
            "Epoch 141/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3936 - accuracy: 0.8242\n",
            "Epoch 142/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3956 - accuracy: 0.8259\n",
            "Epoch 143/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3955 - accuracy: 0.8268\n",
            "Epoch 144/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3923 - accuracy: 0.8283\n",
            "Epoch 145/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3943 - accuracy: 0.8277\n",
            "Epoch 146/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3917 - accuracy: 0.8284\n",
            "Epoch 147/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3950 - accuracy: 0.8245\n",
            "Epoch 148/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4005 - accuracy: 0.8245\n",
            "Epoch 149/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3967 - accuracy: 0.8244\n",
            "Epoch 150/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3988 - accuracy: 0.8236\n",
            "Epoch 151/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3954 - accuracy: 0.8306\n",
            "Epoch 152/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3959 - accuracy: 0.8260\n",
            "Epoch 153/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3974 - accuracy: 0.8263\n",
            "Epoch 154/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3925 - accuracy: 0.8245\n",
            "Epoch 155/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3954 - accuracy: 0.8269\n",
            "Epoch 156/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3956 - accuracy: 0.8244\n",
            "Epoch 157/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3932 - accuracy: 0.8274\n",
            "Epoch 158/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3956 - accuracy: 0.8268\n",
            "Epoch 159/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3992 - accuracy: 0.8259\n",
            "Epoch 160/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3916 - accuracy: 0.8245\n",
            "Epoch 161/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3932 - accuracy: 0.8249\n",
            "Epoch 162/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.3919 - accuracy: 0.8273\n",
            "Epoch 163/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3967 - accuracy: 0.8261\n",
            "Epoch 164/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3928 - accuracy: 0.8272\n",
            "Epoch 165/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3947 - accuracy: 0.8279\n",
            "Epoch 166/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3911 - accuracy: 0.8248\n",
            "Epoch 167/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3889 - accuracy: 0.8259\n",
            "Epoch 168/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3935 - accuracy: 0.8275\n",
            "Epoch 169/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3932 - accuracy: 0.8289\n",
            "Epoch 170/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3914 - accuracy: 0.8244\n",
            "Epoch 171/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3919 - accuracy: 0.8262\n",
            "Epoch 172/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4053 - accuracy: 0.8242\n",
            "Epoch 173/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3942 - accuracy: 0.8275\n",
            "Epoch 174/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3984 - accuracy: 0.8257\n",
            "Epoch 175/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3907 - accuracy: 0.8307\n",
            "Epoch 176/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3907 - accuracy: 0.8306\n",
            "Epoch 177/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3950 - accuracy: 0.8256\n",
            "Epoch 178/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3942 - accuracy: 0.8251\n",
            "Epoch 179/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3941 - accuracy: 0.8258\n",
            "Epoch 180/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.3943 - accuracy: 0.8248\n",
            "Epoch 181/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3939 - accuracy: 0.8306\n",
            "Epoch 182/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3906 - accuracy: 0.8270\n",
            "Epoch 183/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3921 - accuracy: 0.8274\n",
            "Epoch 184/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3975 - accuracy: 0.8248\n",
            "Epoch 185/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3897 - accuracy: 0.8304\n",
            "Epoch 186/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4002 - accuracy: 0.8257\n",
            "Epoch 187/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4039 - accuracy: 0.8204\n",
            "Epoch 188/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3933 - accuracy: 0.8262\n",
            "Epoch 189/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3944 - accuracy: 0.8273\n",
            "Epoch 190/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3951 - accuracy: 0.8310\n",
            "Epoch 191/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3925 - accuracy: 0.8280\n",
            "Epoch 192/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3896 - accuracy: 0.8285\n",
            "Epoch 193/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3925 - accuracy: 0.8296\n",
            "Epoch 194/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.3903 - accuracy: 0.8272\n",
            "Epoch 195/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3886 - accuracy: 0.8280\n",
            "Epoch 196/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3895 - accuracy: 0.8300\n",
            "Epoch 197/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3916 - accuracy: 0.8294\n",
            "Epoch 198/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3950 - accuracy: 0.8284\n",
            "Epoch 199/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3958 - accuracy: 0.8250\n",
            "Epoch 200/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.3913 - accuracy: 0.8284\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "79/79 [==============================] - 0s 2ms/step\n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87      1824\n",
            "           1       0.65      0.69      0.67       689\n",
            "\n",
            "    accuracy                           0.81      2513\n",
            "   macro avg       0.76      0.77      0.77      2513\n",
            "weighted avg       0.82      0.81      0.81      2513\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8654204224276221\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[1566  258]\n",
            " [ 214  475]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8121766812574612\n",
            "\n",
            "13.47261464898935\n",
            "------------------->>>>>>>>>>Fold no =  3\n",
            "Epoch 1/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.5643 - accuracy: 0.7255\n",
            "Epoch 2/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.5274 - accuracy: 0.7253\n",
            "Epoch 3/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.5071 - accuracy: 0.7243\n",
            "Epoch 4/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4907 - accuracy: 0.7240\n",
            "Epoch 5/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4873 - accuracy: 0.7184\n",
            "Epoch 6/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4843 - accuracy: 0.7211\n",
            "Epoch 7/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4819 - accuracy: 0.7249\n",
            "Epoch 8/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4805 - accuracy: 0.7282\n",
            "Epoch 9/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4775 - accuracy: 0.7382\n",
            "Epoch 10/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4805 - accuracy: 0.7392\n",
            "Epoch 11/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4914 - accuracy: 0.7483\n",
            "Epoch 12/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4699 - accuracy: 0.7638\n",
            "Epoch 13/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4630 - accuracy: 0.7784\n",
            "Epoch 14/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4556 - accuracy: 0.7893\n",
            "Epoch 15/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4531 - accuracy: 0.7883\n",
            "Epoch 16/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4517 - accuracy: 0.7958\n",
            "Epoch 17/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4507 - accuracy: 0.7939\n",
            "Epoch 18/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4481 - accuracy: 0.7914\n",
            "Epoch 19/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4434 - accuracy: 0.7961\n",
            "Epoch 20/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4378 - accuracy: 0.8016\n",
            "Epoch 21/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4427 - accuracy: 0.7952\n",
            "Epoch 22/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4426 - accuracy: 0.7981\n",
            "Epoch 23/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4352 - accuracy: 0.8004\n",
            "Epoch 24/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4415 - accuracy: 0.8001\n",
            "Epoch 25/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4308 - accuracy: 0.8055\n",
            "Epoch 26/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4380 - accuracy: 0.8001\n",
            "Epoch 27/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4331 - accuracy: 0.8040\n",
            "Epoch 28/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4360 - accuracy: 0.8016\n",
            "Epoch 29/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4329 - accuracy: 0.8028\n",
            "Epoch 30/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4348 - accuracy: 0.8051\n",
            "Epoch 31/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4297 - accuracy: 0.8043\n",
            "Epoch 32/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4332 - accuracy: 0.8028\n",
            "Epoch 33/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4321 - accuracy: 0.8053\n",
            "Epoch 34/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4298 - accuracy: 0.8086\n",
            "Epoch 35/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4315 - accuracy: 0.8013\n",
            "Epoch 36/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4300 - accuracy: 0.8058\n",
            "Epoch 37/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4278 - accuracy: 0.8055\n",
            "Epoch 38/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4247 - accuracy: 0.8076\n",
            "Epoch 39/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4259 - accuracy: 0.8081\n",
            "Epoch 40/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4269 - accuracy: 0.8101\n",
            "Epoch 41/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4248 - accuracy: 0.8093\n",
            "Epoch 42/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4271 - accuracy: 0.8078\n",
            "Epoch 43/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4234 - accuracy: 0.8101\n",
            "Epoch 44/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4219 - accuracy: 0.8107\n",
            "Epoch 45/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4238 - accuracy: 0.8090\n",
            "Epoch 46/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4294 - accuracy: 0.8030\n",
            "Epoch 47/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4234 - accuracy: 0.8066\n",
            "Epoch 48/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4214 - accuracy: 0.8078\n",
            "Epoch 49/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4250 - accuracy: 0.8084\n",
            "Epoch 50/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4222 - accuracy: 0.8098\n",
            "Epoch 51/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4206 - accuracy: 0.8105\n",
            "Epoch 52/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4243 - accuracy: 0.8084\n",
            "Epoch 53/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4223 - accuracy: 0.8131\n",
            "Epoch 54/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4228 - accuracy: 0.8077\n",
            "Epoch 55/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4239 - accuracy: 0.8089\n",
            "Epoch 56/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4204 - accuracy: 0.8111\n",
            "Epoch 57/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4229 - accuracy: 0.8090\n",
            "Epoch 58/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4169 - accuracy: 0.8139\n",
            "Epoch 59/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4228 - accuracy: 0.8078\n",
            "Epoch 60/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4251 - accuracy: 0.8098\n",
            "Epoch 61/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4153 - accuracy: 0.8147\n",
            "Epoch 62/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4284 - accuracy: 0.8101\n",
            "Epoch 63/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4169 - accuracy: 0.8135\n",
            "Epoch 64/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4212 - accuracy: 0.8107\n",
            "Epoch 65/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4153 - accuracy: 0.8160\n",
            "Epoch 66/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4170 - accuracy: 0.8152\n",
            "Epoch 67/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4185 - accuracy: 0.8113\n",
            "Epoch 68/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4150 - accuracy: 0.8147\n",
            "Epoch 69/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4206 - accuracy: 0.8118\n",
            "Epoch 70/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4166 - accuracy: 0.8139\n",
            "Epoch 71/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4171 - accuracy: 0.8117\n",
            "Epoch 72/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4136 - accuracy: 0.8182\n",
            "Epoch 73/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4183 - accuracy: 0.8123\n",
            "Epoch 74/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4132 - accuracy: 0.8146\n",
            "Epoch 75/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4149 - accuracy: 0.8148\n",
            "Epoch 76/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4129 - accuracy: 0.8141\n",
            "Epoch 77/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4184 - accuracy: 0.8125\n",
            "Epoch 78/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4140 - accuracy: 0.8171\n",
            "Epoch 79/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4134 - accuracy: 0.8180\n",
            "Epoch 80/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4140 - accuracy: 0.8156\n",
            "Epoch 81/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4162 - accuracy: 0.8156\n",
            "Epoch 82/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4237 - accuracy: 0.8117\n",
            "Epoch 83/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4132 - accuracy: 0.8139\n",
            "Epoch 84/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4155 - accuracy: 0.8180\n",
            "Epoch 85/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4175 - accuracy: 0.8130\n",
            "Epoch 86/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4106 - accuracy: 0.8166\n",
            "Epoch 87/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4149 - accuracy: 0.8142\n",
            "Epoch 88/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4104 - accuracy: 0.8141\n",
            "Epoch 89/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4155 - accuracy: 0.8137\n",
            "Epoch 90/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4132 - accuracy: 0.8185\n",
            "Epoch 91/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4100 - accuracy: 0.8169\n",
            "Epoch 92/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4123 - accuracy: 0.8161\n",
            "Epoch 93/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4128 - accuracy: 0.8178\n",
            "Epoch 94/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4124 - accuracy: 0.8140\n",
            "Epoch 95/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4155 - accuracy: 0.8119\n",
            "Epoch 96/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4102 - accuracy: 0.8213\n",
            "Epoch 97/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4099 - accuracy: 0.8192\n",
            "Epoch 98/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4101 - accuracy: 0.8175\n",
            "Epoch 99/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4097 - accuracy: 0.8179\n",
            "Epoch 100/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4149 - accuracy: 0.8169\n",
            "Epoch 101/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4131 - accuracy: 0.8162\n",
            "Epoch 102/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4115 - accuracy: 0.8155\n",
            "Epoch 103/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4091 - accuracy: 0.8184\n",
            "Epoch 104/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4106 - accuracy: 0.8194\n",
            "Epoch 105/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4150 - accuracy: 0.8156\n",
            "Epoch 106/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4113 - accuracy: 0.8165\n",
            "Epoch 107/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4079 - accuracy: 0.8210\n",
            "Epoch 108/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4117 - accuracy: 0.8166\n",
            "Epoch 109/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4109 - accuracy: 0.8182\n",
            "Epoch 110/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4104 - accuracy: 0.8178\n",
            "Epoch 111/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4149 - accuracy: 0.8159\n",
            "Epoch 112/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4113 - accuracy: 0.8170\n",
            "Epoch 113/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4093 - accuracy: 0.8166\n",
            "Epoch 114/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4126 - accuracy: 0.8186\n",
            "Epoch 115/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4067 - accuracy: 0.8183\n",
            "Epoch 116/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4111 - accuracy: 0.8160\n",
            "Epoch 117/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4093 - accuracy: 0.8190\n",
            "Epoch 118/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4099 - accuracy: 0.8172\n",
            "Epoch 119/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4112 - accuracy: 0.8178\n",
            "Epoch 120/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4073 - accuracy: 0.8199\n",
            "Epoch 121/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4088 - accuracy: 0.8191\n",
            "Epoch 122/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4086 - accuracy: 0.8205\n",
            "Epoch 123/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4107 - accuracy: 0.8180\n",
            "Epoch 124/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4075 - accuracy: 0.8181\n",
            "Epoch 125/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4111 - accuracy: 0.8184\n",
            "Epoch 126/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4059 - accuracy: 0.8239\n",
            "Epoch 127/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4068 - accuracy: 0.8213\n",
            "Epoch 128/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4115 - accuracy: 0.8172\n",
            "Epoch 129/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4099 - accuracy: 0.8180\n",
            "Epoch 130/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4092 - accuracy: 0.8192\n",
            "Epoch 131/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4102 - accuracy: 0.8170\n",
            "Epoch 132/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4076 - accuracy: 0.8211\n",
            "Epoch 133/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4080 - accuracy: 0.8196\n",
            "Epoch 134/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4070 - accuracy: 0.8206\n",
            "Epoch 135/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4136 - accuracy: 0.8156\n",
            "Epoch 136/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4079 - accuracy: 0.8181\n",
            "Epoch 137/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4087 - accuracy: 0.8184\n",
            "Epoch 138/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4083 - accuracy: 0.8202\n",
            "Epoch 139/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4058 - accuracy: 0.8205\n",
            "Epoch 140/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4086 - accuracy: 0.8178\n",
            "Epoch 141/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4120 - accuracy: 0.8182\n",
            "Epoch 142/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4060 - accuracy: 0.8225\n",
            "Epoch 143/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4098 - accuracy: 0.8189\n",
            "Epoch 144/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4124 - accuracy: 0.8180\n",
            "Epoch 145/200\n",
            "1257/1257 [==============================] - 4s 4ms/step - loss: 0.4085 - accuracy: 0.8220\n",
            "Epoch 146/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4086 - accuracy: 0.8183\n",
            "Epoch 147/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4094 - accuracy: 0.8212\n",
            "Epoch 148/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4057 - accuracy: 0.8197\n",
            "Epoch 149/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4060 - accuracy: 0.8232\n",
            "Epoch 150/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4096 - accuracy: 0.8183\n",
            "Epoch 151/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4078 - accuracy: 0.8199\n",
            "Epoch 152/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4088 - accuracy: 0.8210\n",
            "Epoch 153/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4127 - accuracy: 0.8198\n",
            "Epoch 154/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4072 - accuracy: 0.8208\n",
            "Epoch 155/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4070 - accuracy: 0.8202\n",
            "Epoch 156/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4123 - accuracy: 0.8175\n",
            "Epoch 157/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4158 - accuracy: 0.8141\n",
            "Epoch 158/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4044 - accuracy: 0.8233\n",
            "Epoch 159/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4041 - accuracy: 0.8209\n",
            "Epoch 160/200\n",
            "1257/1257 [==============================] - 4s 3ms/step - loss: 0.4071 - accuracy: 0.8228\n",
            "Epoch 161/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4089 - accuracy: 0.8185\n",
            "Epoch 162/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4063 - accuracy: 0.8182\n",
            "Epoch 163/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4116 - accuracy: 0.8167\n",
            "Epoch 164/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4097 - accuracy: 0.8186\n",
            "Epoch 165/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4052 - accuracy: 0.8237\n",
            "Epoch 166/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4110 - accuracy: 0.8184\n",
            "Epoch 167/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4058 - accuracy: 0.8199\n",
            "Epoch 168/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4060 - accuracy: 0.8184\n",
            "Epoch 169/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4074 - accuracy: 0.8207\n",
            "Epoch 170/200\n",
            "1257/1257 [==============================] - 9s 7ms/step - loss: 0.4035 - accuracy: 0.8215\n",
            "Epoch 171/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4064 - accuracy: 0.8207\n",
            "Epoch 172/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4029 - accuracy: 0.8216\n",
            "Epoch 173/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4076 - accuracy: 0.8208\n",
            "Epoch 174/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4084 - accuracy: 0.8183\n",
            "Epoch 175/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4020 - accuracy: 0.8237\n",
            "Epoch 176/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4051 - accuracy: 0.8211\n",
            "Epoch 177/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4090 - accuracy: 0.8178\n",
            "Epoch 178/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4066 - accuracy: 0.8215\n",
            "Epoch 179/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4057 - accuracy: 0.8198\n",
            "Epoch 180/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4027 - accuracy: 0.8235\n",
            "Epoch 181/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4035 - accuracy: 0.8223\n",
            "Epoch 182/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4107 - accuracy: 0.8189\n",
            "Epoch 183/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4054 - accuracy: 0.8187\n",
            "Epoch 184/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4056 - accuracy: 0.8204\n",
            "Epoch 185/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4073 - accuracy: 0.8223\n",
            "Epoch 186/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4035 - accuracy: 0.8192\n",
            "Epoch 187/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4069 - accuracy: 0.8206\n",
            "Epoch 188/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4055 - accuracy: 0.8228\n",
            "Epoch 189/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4012 - accuracy: 0.8234\n",
            "Epoch 190/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4023 - accuracy: 0.8227\n",
            "Epoch 191/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4049 - accuracy: 0.8203\n",
            "Epoch 192/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4065 - accuracy: 0.8244\n",
            "Epoch 193/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4040 - accuracy: 0.8204\n",
            "Epoch 194/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4080 - accuracy: 0.8221\n",
            "Epoch 195/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4016 - accuracy: 0.8230\n",
            "Epoch 196/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4046 - accuracy: 0.8202\n",
            "Epoch 197/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4029 - accuracy: 0.8200\n",
            "Epoch 198/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4048 - accuracy: 0.8204\n",
            "Epoch 199/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4068 - accuracy: 0.8242\n",
            "Epoch 200/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4033 - accuracy: 0.8225\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87      1824\n",
            "           1       0.68      0.60      0.63       688\n",
            "\n",
            "    accuracy                           0.81      2512\n",
            "   macro avg       0.77      0.75      0.75      2512\n",
            "weighted avg       0.81      0.81      0.81      2512\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8527968495002041\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[1626  198]\n",
            " [ 276  412]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8113057324840764\n",
            "\n",
            "12.258673693456302\n",
            "------------------->>>>>>>>>>Fold no =  4\n",
            "Epoch 1/200\n",
            "1257/1257 [==============================] - 7s 4ms/step - loss: 0.5690 - accuracy: 0.7259\n",
            "Epoch 2/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.5566 - accuracy: 0.7260\n",
            "Epoch 3/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5528 - accuracy: 0.7260\n",
            "Epoch 4/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5486 - accuracy: 0.7260\n",
            "Epoch 5/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.5460 - accuracy: 0.7260\n",
            "Epoch 6/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5349 - accuracy: 0.7261\n",
            "Epoch 7/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.5289 - accuracy: 0.7260\n",
            "Epoch 8/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5258 - accuracy: 0.7261\n",
            "Epoch 9/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.5203 - accuracy: 0.7312\n",
            "Epoch 10/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.5181 - accuracy: 0.7279\n",
            "Epoch 11/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.5148 - accuracy: 0.7314\n",
            "Epoch 12/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5091 - accuracy: 0.7417\n",
            "Epoch 13/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.5082 - accuracy: 0.7364\n",
            "Epoch 14/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5051 - accuracy: 0.7476\n",
            "Epoch 15/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.5037 - accuracy: 0.7475\n",
            "Epoch 16/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5020 - accuracy: 0.7520\n",
            "Epoch 17/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5037 - accuracy: 0.7492\n",
            "Epoch 18/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.5009 - accuracy: 0.7554\n",
            "Epoch 19/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.5005 - accuracy: 0.7526\n",
            "Epoch 20/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4974 - accuracy: 0.7544\n",
            "Epoch 21/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4930 - accuracy: 0.7598\n",
            "Epoch 22/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4920 - accuracy: 0.7558\n",
            "Epoch 23/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4904 - accuracy: 0.7548\n",
            "Epoch 24/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4905 - accuracy: 0.7570\n",
            "Epoch 25/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4907 - accuracy: 0.7530\n",
            "Epoch 26/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4901 - accuracy: 0.7540\n",
            "Epoch 27/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4834 - accuracy: 0.7569\n",
            "Epoch 28/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4818 - accuracy: 0.7577\n",
            "Epoch 29/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4833 - accuracy: 0.7502\n",
            "Epoch 30/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4798 - accuracy: 0.7552\n",
            "Epoch 31/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4808 - accuracy: 0.7511\n",
            "Epoch 32/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4740 - accuracy: 0.7534\n",
            "Epoch 33/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4773 - accuracy: 0.7567\n",
            "Epoch 34/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4741 - accuracy: 0.7522\n",
            "Epoch 35/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4773 - accuracy: 0.7512\n",
            "Epoch 36/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4731 - accuracy: 0.7518\n",
            "Epoch 37/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4862 - accuracy: 0.7496\n",
            "Epoch 38/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4826 - accuracy: 0.7516\n",
            "Epoch 39/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4810 - accuracy: 0.7562\n",
            "Epoch 40/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4819 - accuracy: 0.7542\n",
            "Epoch 41/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4726 - accuracy: 0.7573\n",
            "Epoch 42/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4850 - accuracy: 0.7517\n",
            "Epoch 43/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4830 - accuracy: 0.7507\n",
            "Epoch 44/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4828 - accuracy: 0.7523\n",
            "Epoch 45/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4836 - accuracy: 0.7543\n",
            "Epoch 46/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4779 - accuracy: 0.7515\n",
            "Epoch 47/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4804 - accuracy: 0.7510\n",
            "Epoch 48/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4728 - accuracy: 0.7541\n",
            "Epoch 49/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4721 - accuracy: 0.7561\n",
            "Epoch 50/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4740 - accuracy: 0.7571\n",
            "Epoch 51/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4746 - accuracy: 0.7550\n",
            "Epoch 52/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4775 - accuracy: 0.7561\n",
            "Epoch 53/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4765 - accuracy: 0.7582\n",
            "Epoch 54/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4792 - accuracy: 0.7529\n",
            "Epoch 55/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4712 - accuracy: 0.7593\n",
            "Epoch 56/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4691 - accuracy: 0.7604\n",
            "Epoch 57/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4702 - accuracy: 0.7598\n",
            "Epoch 58/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4692 - accuracy: 0.7575\n",
            "Epoch 59/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4706 - accuracy: 0.7625\n",
            "Epoch 60/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4683 - accuracy: 0.7656\n",
            "Epoch 61/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4654 - accuracy: 0.7582\n",
            "Epoch 62/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4653 - accuracy: 0.7617\n",
            "Epoch 63/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4681 - accuracy: 0.7648\n",
            "Epoch 64/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4687 - accuracy: 0.7664\n",
            "Epoch 65/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4591 - accuracy: 0.7722\n",
            "Epoch 66/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4662 - accuracy: 0.7721\n",
            "Epoch 67/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4595 - accuracy: 0.7666\n",
            "Epoch 68/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4602 - accuracy: 0.7721\n",
            "Epoch 69/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4617 - accuracy: 0.7715\n",
            "Epoch 70/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4575 - accuracy: 0.7754\n",
            "Epoch 71/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4547 - accuracy: 0.7741\n",
            "Epoch 72/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4602 - accuracy: 0.7694\n",
            "Epoch 73/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4606 - accuracy: 0.7688\n",
            "Epoch 74/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4596 - accuracy: 0.7699\n",
            "Epoch 75/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4604 - accuracy: 0.7734\n",
            "Epoch 76/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4599 - accuracy: 0.7740\n",
            "Epoch 77/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4546 - accuracy: 0.7773\n",
            "Epoch 78/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4487 - accuracy: 0.7760\n",
            "Epoch 79/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4455 - accuracy: 0.7763\n",
            "Epoch 80/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4477 - accuracy: 0.7767\n",
            "Epoch 81/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4432 - accuracy: 0.7789\n",
            "Epoch 82/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4405 - accuracy: 0.7803\n",
            "Epoch 83/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4406 - accuracy: 0.7813\n",
            "Epoch 84/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4438 - accuracy: 0.7780\n",
            "Epoch 85/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4371 - accuracy: 0.7869\n",
            "Epoch 86/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4397 - accuracy: 0.7801\n",
            "Epoch 87/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4327 - accuracy: 0.7849\n",
            "Epoch 88/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4348 - accuracy: 0.7821\n",
            "Epoch 89/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4383 - accuracy: 0.7808\n",
            "Epoch 90/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4311 - accuracy: 0.7857\n",
            "Epoch 91/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4330 - accuracy: 0.7879\n",
            "Epoch 92/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4345 - accuracy: 0.7891\n",
            "Epoch 93/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4334 - accuracy: 0.7846\n",
            "Epoch 94/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4356 - accuracy: 0.7823\n",
            "Epoch 95/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4327 - accuracy: 0.7869\n",
            "Epoch 96/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4315 - accuracy: 0.7888\n",
            "Epoch 97/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4362 - accuracy: 0.7816\n",
            "Epoch 98/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4374 - accuracy: 0.7826\n",
            "Epoch 99/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4252 - accuracy: 0.7852\n",
            "Epoch 100/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4350 - accuracy: 0.7817\n",
            "Epoch 101/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4297 - accuracy: 0.7867\n",
            "Epoch 102/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4276 - accuracy: 0.7826\n",
            "Epoch 103/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4332 - accuracy: 0.7849\n",
            "Epoch 104/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4306 - accuracy: 0.7885\n",
            "Epoch 105/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4290 - accuracy: 0.7858\n",
            "Epoch 106/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4273 - accuracy: 0.7817\n",
            "Epoch 107/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4269 - accuracy: 0.7886\n",
            "Epoch 108/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4282 - accuracy: 0.7839\n",
            "Epoch 109/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4425 - accuracy: 0.7744\n",
            "Epoch 110/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4318 - accuracy: 0.7877\n",
            "Epoch 111/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4232 - accuracy: 0.7948\n",
            "Epoch 112/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4286 - accuracy: 0.7930\n",
            "Epoch 113/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4220 - accuracy: 0.7935\n",
            "Epoch 114/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4201 - accuracy: 0.7973\n",
            "Epoch 115/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4245 - accuracy: 0.7929\n",
            "Epoch 116/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4233 - accuracy: 0.7933\n",
            "Epoch 117/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4265 - accuracy: 0.7929\n",
            "Epoch 118/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4202 - accuracy: 0.7958\n",
            "Epoch 119/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4225 - accuracy: 0.7939\n",
            "Epoch 120/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4179 - accuracy: 0.7999\n",
            "Epoch 121/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4206 - accuracy: 0.7960\n",
            "Epoch 122/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4241 - accuracy: 0.7941\n",
            "Epoch 123/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4152 - accuracy: 0.7947\n",
            "Epoch 124/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4206 - accuracy: 0.7935\n",
            "Epoch 125/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4236 - accuracy: 0.7927\n",
            "Epoch 126/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4198 - accuracy: 0.7930\n",
            "Epoch 127/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4231 - accuracy: 0.7930\n",
            "Epoch 128/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4185 - accuracy: 0.7960\n",
            "Epoch 129/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4195 - accuracy: 0.7970\n",
            "Epoch 130/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4178 - accuracy: 0.8026\n",
            "Epoch 131/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4207 - accuracy: 0.8010\n",
            "Epoch 132/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4209 - accuracy: 0.7957\n",
            "Epoch 133/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4182 - accuracy: 0.7992\n",
            "Epoch 134/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4203 - accuracy: 0.7988\n",
            "Epoch 135/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4224 - accuracy: 0.7964\n",
            "Epoch 136/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4222 - accuracy: 0.8007\n",
            "Epoch 137/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4237 - accuracy: 0.7967\n",
            "Epoch 138/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4135 - accuracy: 0.8047\n",
            "Epoch 139/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4136 - accuracy: 0.8071\n",
            "Epoch 140/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4235 - accuracy: 0.7974\n",
            "Epoch 141/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4220 - accuracy: 0.7986\n",
            "Epoch 142/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4204 - accuracy: 0.8019\n",
            "Epoch 143/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4201 - accuracy: 0.8028\n",
            "Epoch 144/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4200 - accuracy: 0.7977\n",
            "Epoch 145/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4126 - accuracy: 0.8058\n",
            "Epoch 146/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4197 - accuracy: 0.7994\n",
            "Epoch 147/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4254 - accuracy: 0.8032\n",
            "Epoch 148/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4216 - accuracy: 0.8045\n",
            "Epoch 149/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4137 - accuracy: 0.8051\n",
            "Epoch 150/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4120 - accuracy: 0.8047\n",
            "Epoch 151/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4146 - accuracy: 0.8068\n",
            "Epoch 152/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4195 - accuracy: 0.8048\n",
            "Epoch 153/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4134 - accuracy: 0.8087\n",
            "Epoch 154/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4133 - accuracy: 0.8070\n",
            "Epoch 155/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4108 - accuracy: 0.8067\n",
            "Epoch 156/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4155 - accuracy: 0.8098\n",
            "Epoch 157/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4094 - accuracy: 0.8092\n",
            "Epoch 158/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4144 - accuracy: 0.8106\n",
            "Epoch 159/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4116 - accuracy: 0.8105\n",
            "Epoch 160/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4103 - accuracy: 0.8110\n",
            "Epoch 161/200\n",
            "1257/1257 [==============================] - 8s 6ms/step - loss: 0.4121 - accuracy: 0.8084\n",
            "Epoch 162/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4181 - accuracy: 0.8052\n",
            "Epoch 163/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4115 - accuracy: 0.8083\n",
            "Epoch 164/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4295 - accuracy: 0.7909\n",
            "Epoch 165/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4150 - accuracy: 0.8089\n",
            "Epoch 166/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4109 - accuracy: 0.8074\n",
            "Epoch 167/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4112 - accuracy: 0.8135\n",
            "Epoch 168/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4129 - accuracy: 0.8056\n",
            "Epoch 169/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4133 - accuracy: 0.8062\n",
            "Epoch 170/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4098 - accuracy: 0.8126\n",
            "Epoch 171/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4100 - accuracy: 0.8123\n",
            "Epoch 172/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4127 - accuracy: 0.8097\n",
            "Epoch 173/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4065 - accuracy: 0.8100\n",
            "Epoch 174/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4076 - accuracy: 0.8133\n",
            "Epoch 175/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4086 - accuracy: 0.8125\n",
            "Epoch 176/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4124 - accuracy: 0.8100\n",
            "Epoch 177/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4124 - accuracy: 0.8064\n",
            "Epoch 178/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4092 - accuracy: 0.8120\n",
            "Epoch 179/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4104 - accuracy: 0.8103\n",
            "Epoch 180/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4100 - accuracy: 0.8128\n",
            "Epoch 181/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4093 - accuracy: 0.8124\n",
            "Epoch 182/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4106 - accuracy: 0.8103\n",
            "Epoch 183/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4114 - accuracy: 0.8124\n",
            "Epoch 184/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4095 - accuracy: 0.8100\n",
            "Epoch 185/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4091 - accuracy: 0.8130\n",
            "Epoch 186/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4064 - accuracy: 0.8142\n",
            "Epoch 187/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4075 - accuracy: 0.8140\n",
            "Epoch 188/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4083 - accuracy: 0.8124\n",
            "Epoch 189/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4148 - accuracy: 0.8100\n",
            "Epoch 190/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4087 - accuracy: 0.8125\n",
            "Epoch 191/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4094 - accuracy: 0.8126\n",
            "Epoch 192/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4104 - accuracy: 0.8125\n",
            "Epoch 193/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4056 - accuracy: 0.8118\n",
            "Epoch 194/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4134 - accuracy: 0.8120\n",
            "Epoch 195/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4077 - accuracy: 0.8132\n",
            "Epoch 196/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4112 - accuracy: 0.8064\n",
            "Epoch 197/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4065 - accuracy: 0.8110\n",
            "Epoch 198/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4087 - accuracy: 0.8140\n",
            "Epoch 199/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4090 - accuracy: 0.8089\n",
            "Epoch 200/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4076 - accuracy: 0.8119\n",
            "79/79 [==============================] - 0s 4ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88      1824\n",
            "           1       0.71      0.65      0.67       688\n",
            "\n",
            "    accuracy                           0.83      2512\n",
            "   macro avg       0.79      0.77      0.78      2512\n",
            "weighted avg       0.83      0.83      0.83      2512\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8717276589912281\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[1639  185]\n",
            " [ 244  444]]\n",
            "\n",
            "Accuracy for Current Fold: 0.82921974522293\n",
            "\n",
            "16.121311475409836\n",
            "------------------->>>>>>>>>>Fold no =  5\n",
            "Epoch 1/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.5647 - accuracy: 0.7238\n",
            "Epoch 2/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.5195 - accuracy: 0.7337\n",
            "Epoch 3/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.5002 - accuracy: 0.7396\n",
            "Epoch 4/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4939 - accuracy: 0.7460\n",
            "Epoch 5/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4930 - accuracy: 0.7436\n",
            "Epoch 6/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4876 - accuracy: 0.7458\n",
            "Epoch 7/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4889 - accuracy: 0.7554\n",
            "Epoch 8/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4848 - accuracy: 0.7526\n",
            "Epoch 9/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4825 - accuracy: 0.7585\n",
            "Epoch 10/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4816 - accuracy: 0.7600\n",
            "Epoch 11/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4754 - accuracy: 0.7622\n",
            "Epoch 12/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4770 - accuracy: 0.7622\n",
            "Epoch 13/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4731 - accuracy: 0.7634\n",
            "Epoch 14/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4733 - accuracy: 0.7644\n",
            "Epoch 15/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4751 - accuracy: 0.7672\n",
            "Epoch 16/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4757 - accuracy: 0.7629\n",
            "Epoch 17/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4761 - accuracy: 0.7659\n",
            "Epoch 18/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4710 - accuracy: 0.7647\n",
            "Epoch 19/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4749 - accuracy: 0.7671\n",
            "Epoch 20/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4695 - accuracy: 0.7659\n",
            "Epoch 21/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4707 - accuracy: 0.7611\n",
            "Epoch 22/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4652 - accuracy: 0.7696\n",
            "Epoch 23/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4689 - accuracy: 0.7628\n",
            "Epoch 24/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4624 - accuracy: 0.7612\n",
            "Epoch 25/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4654 - accuracy: 0.7656\n",
            "Epoch 26/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4590 - accuracy: 0.7704\n",
            "Epoch 27/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4597 - accuracy: 0.7664\n",
            "Epoch 28/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4586 - accuracy: 0.7638\n",
            "Epoch 29/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4595 - accuracy: 0.7656\n",
            "Epoch 30/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4565 - accuracy: 0.7662\n",
            "Epoch 31/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4563 - accuracy: 0.7650\n",
            "Epoch 32/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4555 - accuracy: 0.7674\n",
            "Epoch 33/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4583 - accuracy: 0.7672\n",
            "Epoch 34/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4541 - accuracy: 0.7702\n",
            "Epoch 35/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4550 - accuracy: 0.7683\n",
            "Epoch 36/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4533 - accuracy: 0.7697\n",
            "Epoch 37/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4553 - accuracy: 0.7668\n",
            "Epoch 38/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4523 - accuracy: 0.7702\n",
            "Epoch 39/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4512 - accuracy: 0.7690\n",
            "Epoch 40/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4522 - accuracy: 0.7702\n",
            "Epoch 41/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4537 - accuracy: 0.7684\n",
            "Epoch 42/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4512 - accuracy: 0.7691\n",
            "Epoch 43/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4483 - accuracy: 0.7675\n",
            "Epoch 44/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4503 - accuracy: 0.7691\n",
            "Epoch 45/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4487 - accuracy: 0.7711\n",
            "Epoch 46/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4492 - accuracy: 0.7682\n",
            "Epoch 47/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4477 - accuracy: 0.7665\n",
            "Epoch 48/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4461 - accuracy: 0.7700\n",
            "Epoch 49/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4455 - accuracy: 0.7737\n",
            "Epoch 50/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4437 - accuracy: 0.7718\n",
            "Epoch 51/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4454 - accuracy: 0.7732\n",
            "Epoch 52/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4455 - accuracy: 0.7720\n",
            "Epoch 53/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4466 - accuracy: 0.7740\n",
            "Epoch 54/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4449 - accuracy: 0.7735\n",
            "Epoch 55/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4417 - accuracy: 0.7770\n",
            "Epoch 56/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4423 - accuracy: 0.7731\n",
            "Epoch 57/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4421 - accuracy: 0.7724\n",
            "Epoch 58/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4398 - accuracy: 0.7761\n",
            "Epoch 59/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4404 - accuracy: 0.7771\n",
            "Epoch 60/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4408 - accuracy: 0.7817\n",
            "Epoch 61/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4339 - accuracy: 0.7886\n",
            "Epoch 62/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4320 - accuracy: 0.7980\n",
            "Epoch 63/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4271 - accuracy: 0.8126\n",
            "Epoch 64/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4269 - accuracy: 0.8050\n",
            "Epoch 65/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4289 - accuracy: 0.8117\n",
            "Epoch 66/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4193 - accuracy: 0.8122\n",
            "Epoch 67/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4216 - accuracy: 0.8119\n",
            "Epoch 68/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4228 - accuracy: 0.8097\n",
            "Epoch 69/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4206 - accuracy: 0.8155\n",
            "Epoch 70/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4191 - accuracy: 0.8135\n",
            "Epoch 71/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4151 - accuracy: 0.8138\n",
            "Epoch 72/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4176 - accuracy: 0.8150\n",
            "Epoch 73/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4197 - accuracy: 0.8122\n",
            "Epoch 74/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4190 - accuracy: 0.8183\n",
            "Epoch 75/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4172 - accuracy: 0.8185\n",
            "Epoch 76/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4158 - accuracy: 0.8154\n",
            "Epoch 77/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4154 - accuracy: 0.8174\n",
            "Epoch 78/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4164 - accuracy: 0.8145\n",
            "Epoch 79/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4139 - accuracy: 0.8163\n",
            "Epoch 80/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4149 - accuracy: 0.8172\n",
            "Epoch 81/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4150 - accuracy: 0.8166\n",
            "Epoch 82/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4156 - accuracy: 0.8180\n",
            "Epoch 83/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4169 - accuracy: 0.8169\n",
            "Epoch 84/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4099 - accuracy: 0.8186\n",
            "Epoch 85/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4060 - accuracy: 0.8205\n",
            "Epoch 86/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4098 - accuracy: 0.8191\n",
            "Epoch 87/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4116 - accuracy: 0.8176\n",
            "Epoch 88/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4076 - accuracy: 0.8205\n",
            "Epoch 89/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4117 - accuracy: 0.8178\n",
            "Epoch 90/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4104 - accuracy: 0.8216\n",
            "Epoch 91/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4058 - accuracy: 0.8204\n",
            "Epoch 92/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4068 - accuracy: 0.8219\n",
            "Epoch 93/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4066 - accuracy: 0.8225\n",
            "Epoch 94/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4067 - accuracy: 0.8213\n",
            "Epoch 95/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4066 - accuracy: 0.8199\n",
            "Epoch 96/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4070 - accuracy: 0.8198\n",
            "Epoch 97/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4061 - accuracy: 0.8224\n",
            "Epoch 98/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4069 - accuracy: 0.8203\n",
            "Epoch 99/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.4065 - accuracy: 0.8218\n",
            "Epoch 100/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4001 - accuracy: 0.8258\n",
            "Epoch 101/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4001 - accuracy: 0.8242\n",
            "Epoch 102/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4018 - accuracy: 0.8265\n",
            "Epoch 103/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4012 - accuracy: 0.8262\n",
            "Epoch 104/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4059 - accuracy: 0.8204\n",
            "Epoch 105/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4021 - accuracy: 0.8273\n",
            "Epoch 106/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3977 - accuracy: 0.8278\n",
            "Epoch 107/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4035 - accuracy: 0.8252\n",
            "Epoch 108/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4071 - accuracy: 0.8187\n",
            "Epoch 109/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4046 - accuracy: 0.8240\n",
            "Epoch 110/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4043 - accuracy: 0.8221\n",
            "Epoch 111/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4016 - accuracy: 0.8245\n",
            "Epoch 112/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4015 - accuracy: 0.8247\n",
            "Epoch 113/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3952 - accuracy: 0.8268\n",
            "Epoch 114/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3980 - accuracy: 0.8262\n",
            "Epoch 115/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3985 - accuracy: 0.8251\n",
            "Epoch 116/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3955 - accuracy: 0.8256\n",
            "Epoch 117/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3962 - accuracy: 0.8263\n",
            "Epoch 118/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3943 - accuracy: 0.8295\n",
            "Epoch 119/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3961 - accuracy: 0.8258\n",
            "Epoch 120/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3978 - accuracy: 0.8230\n",
            "Epoch 121/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3981 - accuracy: 0.8276\n",
            "Epoch 122/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4128 - accuracy: 0.8190\n",
            "Epoch 123/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4037 - accuracy: 0.8253\n",
            "Epoch 124/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4080 - accuracy: 0.8238\n",
            "Epoch 125/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4122 - accuracy: 0.8204\n",
            "Epoch 126/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4107 - accuracy: 0.8216\n",
            "Epoch 127/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4132 - accuracy: 0.8184\n",
            "Epoch 128/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4075 - accuracy: 0.8222\n",
            "Epoch 129/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4030 - accuracy: 0.8257\n",
            "Epoch 130/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4059 - accuracy: 0.8232\n",
            "Epoch 131/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4007 - accuracy: 0.8269\n",
            "Epoch 132/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4038 - accuracy: 0.8271\n",
            "Epoch 133/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.4023 - accuracy: 0.8240\n",
            "Epoch 134/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3986 - accuracy: 0.8245\n",
            "Epoch 135/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4068 - accuracy: 0.8225\n",
            "Epoch 136/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3961 - accuracy: 0.8291\n",
            "Epoch 137/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4005 - accuracy: 0.8265\n",
            "Epoch 138/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3996 - accuracy: 0.8279\n",
            "Epoch 139/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4002 - accuracy: 0.8271\n",
            "Epoch 140/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3964 - accuracy: 0.8280\n",
            "Epoch 141/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4011 - accuracy: 0.8279\n",
            "Epoch 142/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4001 - accuracy: 0.8245\n",
            "Epoch 143/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4013 - accuracy: 0.8248\n",
            "Epoch 144/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4017 - accuracy: 0.8270\n",
            "Epoch 145/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3975 - accuracy: 0.8285\n",
            "Epoch 146/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3966 - accuracy: 0.8273\n",
            "Epoch 147/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3986 - accuracy: 0.8276\n",
            "Epoch 148/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3949 - accuracy: 0.8282\n",
            "Epoch 149/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3963 - accuracy: 0.8285\n",
            "Epoch 150/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.3953 - accuracy: 0.8273\n",
            "Epoch 151/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3989 - accuracy: 0.8298\n",
            "Epoch 152/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3932 - accuracy: 0.8318\n",
            "Epoch 153/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3887 - accuracy: 0.8280\n",
            "Epoch 154/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3933 - accuracy: 0.8282\n",
            "Epoch 155/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3961 - accuracy: 0.8327\n",
            "Epoch 156/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3960 - accuracy: 0.8304\n",
            "Epoch 157/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3887 - accuracy: 0.8309\n",
            "Epoch 158/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3917 - accuracy: 0.8320\n",
            "Epoch 159/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3980 - accuracy: 0.8270\n",
            "Epoch 160/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4006 - accuracy: 0.8274\n",
            "Epoch 161/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3967 - accuracy: 0.8286\n",
            "Epoch 162/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3876 - accuracy: 0.8320\n",
            "Epoch 163/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3931 - accuracy: 0.8303\n",
            "Epoch 164/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3886 - accuracy: 0.8316\n",
            "Epoch 165/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3918 - accuracy: 0.8307\n",
            "Epoch 166/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4127 - accuracy: 0.8191\n",
            "Epoch 167/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.4100 - accuracy: 0.8222\n",
            "Epoch 168/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.4047 - accuracy: 0.8229\n",
            "Epoch 169/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3994 - accuracy: 0.8303\n",
            "Epoch 170/200\n",
            "1257/1257 [==============================] - 7s 6ms/step - loss: 0.3948 - accuracy: 0.8289\n",
            "Epoch 171/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3943 - accuracy: 0.8307\n",
            "Epoch 172/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3899 - accuracy: 0.8333\n",
            "Epoch 173/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3941 - accuracy: 0.8331\n",
            "Epoch 174/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3902 - accuracy: 0.8324\n",
            "Epoch 175/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3888 - accuracy: 0.8305\n",
            "Epoch 176/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3947 - accuracy: 0.8260\n",
            "Epoch 177/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3941 - accuracy: 0.8298\n",
            "Epoch 178/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3974 - accuracy: 0.8285\n",
            "Epoch 179/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3923 - accuracy: 0.8322\n",
            "Epoch 180/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3920 - accuracy: 0.8301\n",
            "Epoch 181/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3933 - accuracy: 0.8327\n",
            "Epoch 182/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3913 - accuracy: 0.8315\n",
            "Epoch 183/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3841 - accuracy: 0.8330\n",
            "Epoch 184/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3971 - accuracy: 0.8259\n",
            "Epoch 185/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.4020 - accuracy: 0.8295\n",
            "Epoch 186/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3916 - accuracy: 0.8324\n",
            "Epoch 187/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3896 - accuracy: 0.8301\n",
            "Epoch 188/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3886 - accuracy: 0.8346\n",
            "Epoch 189/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3957 - accuracy: 0.8293\n",
            "Epoch 190/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3907 - accuracy: 0.8295\n",
            "Epoch 191/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3886 - accuracy: 0.8301\n",
            "Epoch 192/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3911 - accuracy: 0.8316\n",
            "Epoch 193/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3876 - accuracy: 0.8323\n",
            "Epoch 194/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3924 - accuracy: 0.8322\n",
            "Epoch 195/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3901 - accuracy: 0.8303\n",
            "Epoch 196/200\n",
            "1257/1257 [==============================] - 6s 5ms/step - loss: 0.3904 - accuracy: 0.8300\n",
            "Epoch 197/200\n",
            "1257/1257 [==============================] - 6s 4ms/step - loss: 0.3919 - accuracy: 0.8331\n",
            "Epoch 198/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3898 - accuracy: 0.8318\n",
            "Epoch 199/200\n",
            "1257/1257 [==============================] - 7s 5ms/step - loss: 0.3878 - accuracy: 0.8314\n",
            "Epoch 200/200\n",
            "1257/1257 [==============================] - 5s 4ms/step - loss: 0.3887 - accuracy: 0.8313\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "79/79 [==============================] - 0s 3ms/step\n",
            "--------------------------------------------------\n",
            "Detailed classification report for current fold:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.89      0.88      1824\n",
            "           1       0.69      0.61      0.65       688\n",
            "\n",
            "    accuracy                           0.82      2512\n",
            "   macro avg       0.77      0.75      0.76      2512\n",
            "weighted avg       0.81      0.82      0.81      2512\n",
            "\n",
            "\n",
            "Area Under ROC (AUC): 0.8740150703794369\n",
            "\n",
            "Confusion Matrix for current fold: \n",
            "[[1630  194]\n",
            " [ 265  423]]\n",
            "\n",
            "Accuracy for Current Fold: 0.8172770700636943\n",
            "\n",
            "13.411593075277183\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------Average---------------------\n",
            "AUC (Avg. +/- Std.) is  0.859 +/- 0.017\n",
            "Accuracy (Avg. +/- Std.) is  0.812 +/- 0.013\n",
            "Avg. CM is [[433, 217], [255, 1606]]\n",
            "Total for all folds CM is [[2165, 1088], [1277, 8032]]\n",
            "Sensitivity (Avg. +/- Std.) is  0.629 +/- 0.035\n",
            "Specificity (Avg. +/- Std.) is  0.881 +/- 0.017\n",
            "Precision (Avg. +/- Std.) is  0.667 +/- 0.030\n",
            "FOR (Avg. +/- Std.) is  0.137 +/- 0.011\n",
            "DOR (Avg. +/- Std.) is  12.889 +/- 2.246\n"
          ]
        }
      ],
      "source": [
        "i=0                                                                          \n",
        "Accuracy = []                                                               \n",
        "FP = []                                                                     \n",
        "TN = []                                                                    \n",
        "FN = []                                                                    \n",
        "TP = []                                                                      \n",
        "tprs = []                                                                   \n",
        "aucs_ens = []                                                                \n",
        "sn = []                                                                      \n",
        "sp = []                                                                      \n",
        "pr = []                                                                      \n",
        "FOR = []                                                                     \n",
        "DOR = [] \n",
        "iterator=0\n",
        "mean_fpr = np.linspace(0, 1, 100)\n",
        "activation=\"relu\"\n",
        "batch_size=8\n",
        "epochs=200\n",
        "learn_rate=.001\n",
        "dropout_rate=0.6\n",
        "init=\"normal\"\n",
        "neuron1,neuron2,neuron3,neuron4=64,16,64,64\n",
        "print(activation,batch_size,epochs,learn_rate,dropout_rate,init,\n",
        "    neuron1,neuron2,neuron3,neuron4)\n",
        "\n",
        "\n",
        "\n",
        "for train_index, test_index in kf.split(X_Data,Y_Lavel):                 \n",
        "  print('------------------->>>>>>>>>>Fold no = ',i+1)\n",
        "  X_Train, X_Test = X_Data[train_index], X_Data[test_index]               \n",
        "  Y_Train, Y_Test = Y_Lavel[train_index], Y_Lavel[test_index]           \n",
        "\n",
        "\n",
        "  Y_Train_1Hot = to_categorical(Y_Train,2)                                \n",
        "  Y_Test_1Hot = to_categorical(Y_Test,2)                                  \n",
        "\n",
        "  model =nn_opt(activation,                                               \n",
        "            dropout_rate,\n",
        "            init,\n",
        "            learn_rate)    \n",
        "  np.random.seed(6)\n",
        "  model.fit(x=X_Train,                                                     \n",
        "            y=Y_Train_1Hot,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            shuffle=False,\n",
        "            verbose=1)\n",
        "    \n",
        "  probas_ = model.predict(X_Test)                                           \n",
        "\n",
        "  y_pred = np.argmax(model.predict(X_Test), axis=1)                      \n",
        "\n",
        "\n",
        "  tn, fp, fn, tp, roc_auc, fpr, tpr = metrics (y_true = Y_Test,           \n",
        "                                              y_pred = y_pred,\n",
        "                                              probas_ = probas_)\n",
        "  tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "  tprs[-1][0] = 0.0\n",
        "  aucs_ens.append(roc_auc)\n",
        "  plot_Current_ROC (fpr,tpr,iterator,roc_auc)                             \n",
        "  iterator += 1\n",
        "  TN.append(tn)\n",
        "  FP.append(fp)\n",
        "  FN.append(fn)\n",
        "  TP.append(tp)\n",
        "  Accuracy.append(accuracy_score(Y_Test, y_pred))\n",
        "  sn.append(tp/(tp+fn))\n",
        "  sp.append(tn/(fp+tn))\n",
        "  pr.append(tp/(tp+fp))\n",
        "  FOR.append(fn/(tn+fn))\n",
        "  DOR.append((tp*tn)/(fp*fn))\n",
        "  print((tp*tn)/(fp*fn))\n",
        "  i+=1\n",
        "\n",
        "average_ROC(mean_fpr,tprs,aucs_ens,TP,TN,FP,FN)                             \n",
        "average_performance(aucs_ens,Accuracy,TP,TN,FP,FN)                            \n",
        "print(\"Sensitivity (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(sn),np.std(sn)))\n",
        "print(\"Specificity (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(sp),np.std(sp)))\n",
        "print(\"Precision (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(pr),np.std(pr)))\n",
        "print(\"FOR (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(FOR),np.std(FOR)))\n",
        "print(\"DOR (Avg. +/- Std.) is  %0.3f +/- %0.3f\" %(np.mean(DOR),np.std(DOR)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0"
      ],
      "metadata": {
        "id": "Tepa-wszJ6cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}